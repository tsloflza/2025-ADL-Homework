{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114453,"databundleVersionId":13653249,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi\n!pip install datasets evaluate torch accelerate tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:07:24.277534Z","iopub.execute_input":"2025-09-15T08:07:24.278212Z","iopub.status.idle":"2025-09-15T08:07:27.817354Z","shell.execute_reply.started":"2025-09-15T08:07:24.278189Z","shell.execute_reply":"2025-09-15T08:07:27.816354Z"}},"outputs":[{"name":"stdout","text":"Mon Sep 15 08:07:24 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0             27W /  250W |       3MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"#!/usr/bin/env python\n# Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nFine-tuning a ü§ó Transformers model on multiple choice relying on the accelerate library without using a Trainer.\n\"\"\"\n# You can also adapt this script on your own multiple choice task. Pointers for this are left as comments.\n\nimport argparse\nimport json\nimport math\nimport os\nimport random\nfrom itertools import chain\nfrom pathlib import Path\nfrom types import SimpleNamespace\n\nimport datasets\nimport evaluate\nimport torch\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nimport transformers\nfrom transformers import (\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    AutoConfig,\n    AutoModelForMultipleChoice,\n    AutoTokenizer,\n    DataCollatorForMultipleChoice,\n    SchedulerType,\n    default_data_collator,\n    get_scheduler,\n)\nfrom transformers.utils import check_min_version, send_example_telemetry","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:07:27.818831Z","iopub.execute_input":"2025-09-15T08:07:27.819117Z","iopub.status.idle":"2025-09-15T08:07:27.825719Z","shell.execute_reply.started":"2025-09-15T08:07:27.819088Z","shell.execute_reply":"2025-09-15T08:07:27.824950Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"args = SimpleNamespace(\n    train_file=\"/kaggle/input/ntu-adl-2025-hw-1/train.json\",\n    validation_file=\"/kaggle/input/ntu-adl-2025-hw-1/valid.json\",\n    context_file=\"/kaggle/input/ntu-adl-2025-hw-1/context.json\",\n    max_seq_length=512,\n    pad_to_max_length=False,\n    model_name_or_path=\"bert-base-chinese\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    learning_rate=3e-5,\n    num_train_epochs=1,\n    max_train_steps=None,\n    gradient_accumulation_steps=2,\n    lr_scheduler_type=SchedulerType.LINEAR, # choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n    output_dir=\"/kaggle/working/\",\n    seed=1234,\n)\n\nprint(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:07:27.826488Z","iopub.execute_input":"2025-09-15T08:07:27.826656Z","iopub.status.idle":"2025-09-15T08:07:27.843350Z","shell.execute_reply.started":"2025-09-15T08:07:27.826642Z","shell.execute_reply":"2025-09-15T08:07:27.842645Z"}},"outputs":[{"name":"stdout","text":"namespace(train_file='/kaggle/input/ntu-adl-2025-hw-1/train.json', validation_file='/kaggle/input/ntu-adl-2025-hw-1/valid.json', test_file='/kaggle/input/ntu-adl-2025-hw-1/test.json', context_file='/kaggle/input/ntu-adl-2025-hw-1/context.json', max_seq_length=512, pad_to_max_length=False, model_name_or_path='bert-base-chinese', per_device_train_batch_size=1, per_device_eval_batch_size=1, learning_rate=3e-05, num_train_epochs=1, max_train_steps=None, gradient_accumulation_steps=2, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, output_dir='/kaggle/working/', seed=1234)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Establish accelerator\naccelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)\n\n# Set the training seed now.\nif args.seed is not None:\n    set_seed(args.seed)\n\n# Handle the repository creation\nif accelerator.is_main_process:\n    if args.output_dir is not None:\n        os.makedirs(args.output_dir, exist_ok=True)\naccelerator.wait_for_everyone()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:07:27.844689Z","iopub.execute_input":"2025-09-15T08:07:27.844916Z","iopub.status.idle":"2025-09-15T08:07:27.862412Z","shell.execute_reply.started":"2025-09-15T08:07:27.844900Z","shell.execute_reply":"2025-09-15T08:07:27.861821Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Get the datasets\n\nwith open(args.context_file, \"r\", encoding=\"utf-8\") as f:\n    contexts = json.load(f)\n\ndef load_paragraph_selection(file_path, contexts):\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        examples = json.load(f)\n\n    data = {\n        \"id\": [],\n        \"question\": [],\n        \"paragraphs\": [],\n        \"label\": [],        # Ê≠£Ëß£ÊÆµËêΩÂú® paragraphs Ë£°ÁöÑÁ¨¨ÂπæÂÄã\n    }\n\n    for ex in examples:\n        qid = ex[\"id\"]\n        question = ex[\"question\"]\n        para_ids = ex[\"paragraphs\"]\n        relevant_id = ex[\"relevant\"]\n\n        para_texts = [contexts[pid] for pid in para_ids]\n        label = para_ids.index(relevant_id)\n\n        data[\"id\"].append(qid)\n        data[\"question\"].append(question)\n        data[\"paragraphs\"].append(para_texts)\n        data[\"label\"].append(label)\n\n    return datasets.Dataset.from_dict(data)\n\n# Load train/valid\ndataset_splits = {}\nif args.train_file is not None:\n    dataset_splits[\"train\"] = load_paragraph_selection(args.train_file, contexts)\nif args.validation_file is not None:\n    dataset_splits[\"validation\"] = load_paragraph_selection(args.validation_file, contexts)\n\nraw_datasets = datasets.DatasetDict(dataset_splits)\n\nprint(raw_datasets)\nprint(raw_datasets[\"train\"][0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:07:27.863256Z","iopub.execute_input":"2025-09-15T08:07:27.863599Z","iopub.status.idle":"2025-09-15T08:07:29.542778Z","shell.execute_reply.started":"2025-09-15T08:07:27.863582Z","shell.execute_reply":"2025-09-15T08:07:29.542098Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'question', 'paragraphs', 'label'],\n        num_rows: 21714\n    })\n    validation: Dataset({\n        features: ['id', 'question', 'paragraphs', 'label'],\n        num_rows: 3009\n    })\n})\n{'id': '593f14f960d971e294af884f0194b3a7', 'question': 'ËàçÊú¨ÂíåË™∞ÁöÑÊï∏ÊìöËÉΩÊé®ÁÆóÂá∫ÈÄ£ÊòüÁöÑÊÅÜÊòüÁöÑË≥™ÈáèÔºü', 'paragraphs': ['1930Âπ¥ÔºåÂç∞Â∫¶Áâ©ÁêÜÂ≠∏ÂÆ∂ËòáÂ∏ÉÊãâÈ¶¨Â∞ºÊèö¬∑Èå¢Âæ∑ÊãâÂ°ûÂç°Ê†πÊìöÂª£Áæ©Áõ∏Â∞çË´ñË®àÁÆóÂá∫Ë≥™ÈáèÂ§ßÊñº1.4ÂÄçÂ§™ÈôΩË≥™ÈáèÁöÑÈùûËΩâÂãïÊòüÈ´îÊúÉÂõ†ÈáçÂäõÂ°åÁ∏ÆÊàêÁÇ∫ÈõªÂ≠êÁ∞°‰ΩµÊÖã„ÄÇÊÑõ‰∏ÅÈ†ìÈõñÁÑ∂Âú®ÁêÜË´ñ‰∏äÊîØÊåÅÈªëÊ¥ûÂ≠òÂú®ÁöÑÂèØËÉΩÊÄßÔºå‰ΩÜÂêåÊôÇË™çÁÇ∫Èå¢Âæ∑ÊãâÂ°ûÂç°ÁöÑËßÄÈªû‰∫ãÂØ¶‰∏ä‰∏çËÉΩÊàêÁ´ãÔºå‰ªñË™çÁÇ∫„ÄåÊáâÁï∂ÊúâÊüêÁ®ÆËá™ÁÑ∂ÂÆöÂæãÈòªÊ≠¢ÊÅÜÊòüÂá∫ÁèæÈÄôÁ®ÆËçíÂîêÁöÑË°åÁÇ∫„Äç„ÄÇÁï∂ÊôÇÁöÑÁâ©ÁêÜÂ≠∏ÂÆ∂Â¶ÇÊ≥¢ËÄ≥„ÄÅ‰∫®Âà©¬∑ÁæÖÁ¥†Á≠â‰∫∫ÈÉΩË¥äÂêåÈå¢Âæ∑ÊãâÂ°ûÂç°ÁöÑÁêÜË´ñÔºå‰ΩÜÂá∫ÊñºÊÑõ‰∏ÅÈ†ìËÅ≤ÊúõÁöÑÂéüÂõ†Ôºå‰ªñÂÄë‰∏¶Ê≤íÊúâÂÖ¨ÈñãÂ∞çÈå¢Âæ∑ÊãâÂ°ûÂç°Ë°®Á§∫ÊîØÊåÅ„ÄÇ‰∏çÈÅéÂæûÊüêÁ®ÆÊÑèÁæ©‰∏äË™™ÔºåÊÑõ‰∏ÅÈ†ì‰πüÊòØÊ≠£Á¢∫ÁöÑÔºåÁï∂ÊÅÜÊòüË≥™ÈáèÂ§ßÊñºÈå¢Âæ∑ÊãâÂ°ûÂç°Ê•µÈôêÂæåÔºåÁ¢∫ÂØ¶‰ªçÁÑ∂ÊúÉÊúâ‰∏≠Â≠êÁ∞°‰ΩµÂ£ìÂäõÈòªÊ≠¢ÊÅÜÊòüÁπºÁ∫åÂ°åÁ∏Æ„ÄÇÂà∞‰∫Ü1939Âπ¥ÔºåÁæéÂúãÁâ©ÁêÜÂ≠∏ÂÆ∂ÁæÖ‰ºØÁâπ¬∑Ê≠êÊú¨Êµ∑ÈªòÁ≠â‰∫∫Êé®ÁÆó‰∫ÜÈÄôÁ®ÆÊÉÖÂΩ¢ÁöÑÊÅÜÊòüË≥™Èáè‰∏äÈôêÔºåÈÄôÂÄãÊ•µÈôêË¢´Á®±‰ΩúÊâòÁàæÊõº-Ê≠êÊú¨Êµ∑Èªò-Ê≤ÉÁàæÁßëÂ§´Ê•µÈôê„ÄÇÁï∂‰ªäÁöÑÂ§©È´îÁâ©ÁêÜÂ≠∏ÂÆ∂ÊôÆÈÅçË™çÁÇ∫ÔºåÈô§ÈùûÊúâÂ¶ÇÊú™Áü•ÁöÑÂ§∏ÂÖãÁ∞°‰ΩµÂ£ìÂäõ‰∏ÄÈ°ûÂõ†Á¥†ÁöÑÂ≠òÂú®ÔºåË≥™ÈáèÂ§ßÊñºÊâòÁàæÊõº-Ê≠êÊú¨Êµ∑Èªò-Ê≤ÉÁàæÁßëÂ§´Ê•µÈôêÁöÑÊÅÜÊòüÂ∞áÊúÄÁµÇÊúÉÂ°åÁ∏ÆÁÇ∫Èå¢Âæ∑ÊãâÂ°ûÂç°ÊâÄÈ†êË®ÄÁöÑÈªëÊ¥û„ÄÇÂç≥‰ΩøÂ¶ÇÊ≠§ÔºåÂè≤Áì¶Ë•øËß£‰ΩúÁÇ∫Áï∂ÊôÇËÉΩÂ§†ÊèèËø∞ÈªëÊ¥ûË°åÁÇ∫ÁöÑÂîØ‰∏ÄÁ≤æÁ¢∫Ëß£ÔºåÁî±ÊñºÂÖ∑Êúâ‰∏Ä‰∫õËÆì‰∫∫Áúã‰ºº‰∏çÂ§ßÂÑ™ÁæéÁöÑÊÄßË≥™‰ª•ÂèäÈõ£‰ª•ËàáÂØ¶È©óËßÄÊ∏¨Áõ∏ËÅØÁπ´Ôºå‰∏ÄÁõ¥Ê≤íÊúâÈÄ≤ÂÖ•‰∏ªÊµÅÁâ©ÁêÜÂ≠∏Á†îÁ©∂ÁöÑË¶ñÈáéÔºåÈóúÊñºÈªëÊ¥ûÁöÑÁêÜË´ñ‰πÉËá≥Êï¥ÂÄãÂª£Áæ©Áõ∏Â∞çË´ñÈ†òÂüüÁöÑÁ†îÁ©∂Áî±Ê≠§Êì±ÁΩÆ‰∫Ü‰∫åÂçÅÂπ¥‰πã‰πÖ„ÄÇ', 'ÂøÉÁêÜÂ≠∏ÊòØÂê¶ÁÇ∫Ëá™ÁÑ∂ÁßëÂ≠∏ÁöÑÁØÑÂúçÔºåÁõÆÂâç‰πüÂ∞öÂ≠òÁà≠Ë≠∞Ôºå‰∏ÄËà¨ËºÉÂª£ÁÇ∫Êé•ÂèóÁöÑË™™Ê≥ïÊòØÂøÉÁêÜÂ≠∏ÂêåÊôÇÂåÖÂê´Âú®Ëá™ÁÑ∂ÁßëÂ≠∏ËàáÁ§æÊúÉÁßëÂ≠∏ÁöÑÁØÑÁñá‰πã‰∏≠„ÄÇËá™ÁÑ∂ÁßëÂ≠∏ÁöÑÊ†πÊú¨ÁõÆÁöÑÂú®ÊñºÂ∞ãÊâæÈö±ËóèÂú®Ëá™ÁÑ∂ÁèæË±°ËÉåÂæåÁöÑË¶èÂæãÔºå‰ΩÜÊòØËá™ÁÑ∂ÁßëÂ≠∏ÁöÑÂ∑•‰ΩúÂ∞ö‰∏çÂåÖÊã¨Á†îÁ©∂ÁÇ∫‰ªÄÈ∫ºÊúÉÂ≠òÂú®ÈÄô‰∫õË¶èÂæã„ÄÇËá™ÁÑ∂ÁßëÂ≠∏Ë™çÁÇ∫Ë∂ÖËá™ÁÑ∂ÁöÑ„ÄÅÈö®ÊÑèÁöÑÂíåËá™Áõ∏ÁüõÁõæÁöÑÁèæË±°ÊòØ‰∏çÂ≠òÂú®ÁöÑ„ÄÇËá™ÁÑ∂ÁßëÂ≠∏ÁöÑÊúÄÈáçË¶ÅÁöÑÂÖ©ÂÄãÊîØÊü±ÊòØËßÄÂØüÂíåÈÇèËºØÊé®ÁêÜ„ÄÇÁî±Â∞çËá™ÁÑ∂ÁöÑËßÄÂØüÂíåÈÇèËºØÊé®ÁêÜËá™ÁÑ∂ÁßëÂ≠∏ÂèØ‰ª•ÂºïÂ∞éÂá∫Â§ßËá™ÁÑ∂‰∏≠ÁöÑË¶èÂæã„ÄÇÂÅáÂ¶ÇËßÄÂØüÁöÑÁèæË±°ËàáË¶èÂæãÁöÑÈ†êË®Ä‰∏çÂêåÔºåÈÇ£È∫ºË¶ÅÈ∫ºÊòØÂõ†ÁÇ∫ËßÄÂØü‰∏≠ÊúâÈåØË™§ÔºåË¶ÅÈ∫ºÊòØÂõ†ÁÇ∫Ëá≥Ê≠§ÁÇ∫Ê≠¢Ë¢´Ë™çÁÇ∫ÊòØÊ≠£Á¢∫ÁöÑË¶èÂæãÊòØÈåØË™§ÁöÑ„ÄÇ‰∏ÄÂÄãË∂ÖËá™ÁÑ∂Âõ†Á¥†ÊòØ‰∏çÂ≠òÂú®ÁöÑ„ÄÇ', 'ÈÄôÊòØ‰∏ÄÂÄãÈï∑‰πÖ‰ª•‰æÜÂ∞±Â≠òÂú®ÁöÑÂÅáË®≠ÔºåÂ§ßÂ§öÊï∏ÁöÑÊÅÜÊòüÈÉΩÊòØÈï∑ÊúüËôïÂú®ÁâπÂÆöÈáçÂäõÂ†¥ÁöÑÂ§öÊòüÊàñËÅØÊòüÁ≥ªÁµ±„ÄÇÁâπÂà•ÊòØË®±Â§öÂ§ßË≥™ÈáèÁöÑÊ≠êÂíåÈÄºÂûãÊÅÜÊòüÔºåÊúâ80%Ë¢´Ë™çÁÇ∫ÊòØÂ§öÊòüÁ≥ªÁµ±ÁöÑ‰∏ÄÈÉ®ÂàÜ„ÄÇÁÑ∂ËÄåÔºåË≥™ÈáèË∂ä‰ΩéÁöÑÊÅÜÊòüÔºåÂñÆÁç®Â≠òÂú®ÁöÑÊØî‰æãÈ°ØÁÑ∂Ë∂äÈ´òÔºåÂè™Êúâ25%ÁöÑÁ¥ÖÁüÆÊòüË¢´ÁôºÁèæÊúâ‰º¥Êòü„ÄÇÂõ†ÁÇ∫85%ÁöÑÊÅÜÊòüÊòØÁ¥ÖÁüÆÊòüÔºåÊâÄ‰ª•Âú®ÈäÄÊ≤≥Á≥ªÂÖßÂ§öÊï∏ÁöÑÊÅÜÊòüÈÉΩÊòØÂñÆÁç®Ë™ïÁîüÁöÑ„ÄÇÊÅÜÊòüÂú®ÂÆáÂÆô‰∏≠ÁöÑÂàÜÂ∏ÉÊòØ‰∏çÂùáÂãªÁöÑÔºå‰∏¶‰∏îÈÄöÂ∏∏ÈÉΩÊòØÊàêÁæ§ÁöÑËàáÊòüÈöõÈñìÁöÑÊ∞£È´î„ÄÅÂ°µÂüÉ‰∏ÄËµ∑Â≠òÂú®ÊñºÊòüÁ≥ª‰∏≠„ÄÇ‰∏ÄÂÄãÂÖ∏ÂûãÁöÑÊòüÁ≥ªÊìÅÊúâÊï∏ÂçÉÂÑÑÈ°ÜÁöÑÊÅÜÊòüÔºåËÄåÂú®ÂèØËßÄÊ∏¨ÂÆáÂÆô‰∏≠ÁöÑÊòüÁ≥ªÊï∏ÈáèË∂ÖÈÅé‰∏ÄÂçÉÂÑÑÂÄã„ÄÇ2010Âπ¥Â∞çÊÅÜÊòüÊï∏ÈáèÁöÑ‰º∞Ë®àÊòØÂú®ÂèØËßÄÊ∏¨ÂÆáÂÆô‰∏≠Êúâ3000ÂûìÈ°Ü„ÄÇÂÑòÁÆ°‰∫∫ÂÄëÂæÄÂæÄË™çÁÇ∫ÊÅÜÊòüÂÉÖÂ≠òÂú®ÊñºÊòüÁ≥ª‰∏≠Ôºå‰ΩÜÊòüÁ≥ªÈöõÁöÑÊÅÜÊòüÂ∑≤Á∂ìË¢´ÁôºÁèæ‰∫Ü„ÄÇ', 'Âú®19‰∏ñÁ¥ÄÈõôÊòüËßÄÊ∏¨ÊâÄÁç≤ÂæóÁöÑÊàêÂ∞±‰ΩøÈáçË¶ÅÊÄß‰πüÂ¢ûÂä†‰∫Ü„ÄÇÂú®1834Âπ¥ÔºåÁôΩÂ°ûÁàæËßÄÊ∏¨Âà∞Â§©ÁãºÊòüËá™Ë°åÁöÑËÆäÂåñÔºåÂõ†ËÄåÊé®Ê∏¨Êúâ‰∏ÄÈ°ÜÈö±ËóèÁöÑ‰º¥ÊòüÔºõÊÑõÂæ∑ËèØ¬∑ÁöÆÂÖãÊûóÂú®1899Âπ¥ËßÄÊ∏¨ÈñãÈôΩÈÄ±ÊúüÊÄßÂàÜË£ÇÁöÑÂÖâË≠úÁ∑öÊôÇÁôºÁèæÁ¨¨‰∏ÄÈ°ÜÂÖâË≠úÈõôÊòüÔºåÈÄ±ÊúüÊòØ104Â§©„ÄÇÂ§©ÊñáÂ≠∏ÂÆ∂ÊñØÁâπÈ≠ØÁ∂≠ÂíåËàçÊú¨¬∑Ë°õÊñØÈáå¬∑‰ºØÁ¥çÂßÜ‰ªîÁ¥∞ÁöÑËßÄÂØüÂíåÊî∂ÈõÜ‰∫ÜË®±Â§öËÅØÊòüÁöÑË≥áÊñôÔºå‰ΩøÂæóÂèØ‰ª•ÂæûË¢´Á¢∫ÂÆöÁöÑËªåÈÅìË¶ÅÁ¥†Êé®ÁÆóÂá∫ÊÅÜÊòüÁöÑË≥™Èáè„ÄÇÁ¨¨‰∏ÄÂÄãÁç≤ÂæóËß£Á≠îÁöÑÊòØ1827Âπ¥Áî±Ëè≤Âà©ÂÖãÊñØ¬∑Ëñ©Áì¶ÈáåÈÄèÈÅéÊúõÈÅ†Èè°ÁöÑËßÄÊ∏¨ÂæóÂà∞ÁöÑËÅØÊòüËªåÈÅì„ÄÇÂ∞çÊÅÜÊòüÁöÑÁßëÂ≠∏Á†îÁ©∂Âú®20‰∏ñÁ¥ÄÁç≤ÂæóÂø´ÈÄüÁöÑÈÄ≤Â±ïÔºåÁõ∏ÁâáÊàêÁÇ∫Â§©ÊñáÂ≠∏‰∏äÂæàÊúâÂÉπÂÄºÁöÑÂ∑•ÂÖ∑„ÄÇÂç°Áàæ¬∑Âè≤Áì¶Ë•øÁôºÁèæÁ∂ìÁî±ÊØîËºÉË¶ñÊòüÁ≠âÂíåÊîùÂΩ±ÊòüÁ≠âÁöÑÂ∑ÆÂà•ÔºåÂèØ‰ª•ÂæóÂà∞ÊÅÜÊòüÁöÑÈ°èËâ≤ÂíåÂÆÉÁöÑÊ∫´Â∫¶„ÄÇ1921Âπ¥ÔºåÂÖâÈõªÂÖâÂ∫¶Ë®àÁöÑÁôºÂ±ïÂèØ‰ª•Âú®‰∏çÂêåÁöÑÊ≥¢Èï∑ÈñìÈöî‰∏äÈùûÂ∏∏Á≤æÂØÜÁöÑÊ∏¨ÈáèÊòüÁ≠â„ÄÇÈòøÁàæ‰ºØÁâπ¬∑ÈÇÅÂÖãÁîüÂú®ËôéÂÖãÊúõÈÅ†Èè°Á¨¨‰∏ÄÊ¨°‰ΩøÁî®Âπ≤Ê∂âÂÑÄÊ∏¨ÈáèÂá∫ÊÅÜÊòüÁöÑÁõ¥Âæë„ÄÇ'], 'label': 3}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# 1. ËºâÂÖ• config\nconfig = AutoConfig.from_pretrained(args.model_name_or_path)\n\n# 2. ËºâÂÖ• tokenizer\ntokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n\n# 3. ËºâÂÖ• model\nmodel = AutoModelForMultipleChoice.from_pretrained(\n    args.model_name_or_path,\n    from_tf=bool(\".ckpt\" in args.model_name_or_path)\n)\n\n# 4. Ë™øÊï¥ embedding Â§ßÂ∞èÔºàÈÅøÂÖç tokenizer Êñ∞Â¢ûÂ≠óÂÖ∏ÈÄ†Êàê index errorÔºâ\nembedding_size = model.get_input_embeddings().weight.shape[0]\nif len(tokenizer) > embedding_size:\n    model.resize_token_embeddings(len(tokenizer))\n\n# 5. padding ÊñπÂºè\npadding = \"max_length\" if args.pad_to_max_length else False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:09:20.959655Z","iopub.execute_input":"2025-09-15T08:09:20.959946Z","iopub.status.idle":"2025-09-15T08:09:24.991579Z","shell.execute_reply.started":"2025-09-15T08:09:20.959926Z","shell.execute_reply":"2025-09-15T08:09:24.991028Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"569745fec5a241ebb7fe7dfc789aa8be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6d488ccded45df93a623c3fdaf34e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd3d08df2c8b41efaf45c37cc69b4f81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e8eac239bf246798142084c23a9839a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"228cf09f089b418999d1f7a7a6f6f5e7"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# --- Preprocessing ---\ndef preprocess_function(examples):\n    questions = examples[\"question\"]             # list[str]\n    paragraphs_list = examples[\"paragraphs\"]     # list[list[str]] (ÊØèÂÄãÊ®£Êú¨ÂõõÂÄãÊÆµËêΩÊñáÂ≠ó)\n    labels = examples[\"label\"]                   # list[int] (Ê≠£Á¢∫Á¥¢Âºï 0~3)\n\n    first_sentences = []\n    second_sentences = []\n    new_labels = []\n\n    for q, paras, label in zip(questions, paragraphs_list, labels):\n        # ÂïèÈ°åÈáçË§áÂõõÊ¨°\n        first_sentences.extend([q] * 4)\n        # ÊÆµËêΩÊñáÂ≠ó\n        second_sentences.extend(paras)\n        # label ‰øùÁïô\n        new_labels.append(label)\n\n    # Tokenize\n    tokenized_examples = tokenizer(\n        first_sentences,\n        second_sentences,\n        max_length=args.max_seq_length,\n        padding=padding,\n        truncation=True,\n    )\n\n    # Un-flatten ‚Üí [batch_size, 4, seq_len]\n    tokenized_inputs = {\n        k: [v[i:i + 4] for i in range(0, len(v), 4)]\n        for k, v in tokenized_examples.items()\n    }\n    tokenized_inputs[\"labels\"] = new_labels\n    return tokenized_inputs\n\n\nwith accelerator.main_process_first():\n    processed_datasets = raw_datasets.map(\n        preprocess_function, \n        batched=True, \n        remove_columns=raw_datasets[\"train\"].column_names\n    )\n\ntrain_dataset = processed_datasets[\"train\"]\neval_dataset = processed_datasets[\"validation\"]\n\n# --- DataLoaders ---\nif args.pad_to_max_length:\n    data_collator = default_data_collator\nelse:\n    if accelerator.mixed_precision == \"fp8\":\n        pad_to_multiple_of = 16\n    elif accelerator.mixed_precision != \"no\":\n        pad_to_multiple_of = 8\n    else:\n        pad_to_multiple_of = None\n    data_collator = DataCollatorForMultipleChoice(\n        tokenizer, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\"\n    )\n\ntrain_dataloader = DataLoader(\n    train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n)\neval_dataloader = DataLoader(\n    eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size\n)\n\n# --- Optimizer ---\nno_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [\n    {\n        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n    {\n        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n        \"weight_decay\": 0.0,\n    },\n]\noptimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\n# --- Accelerator ---\ndevice = accelerator.device\nmodel.to(device)\n\n# Scheduler\noverrode_max_train_steps = False\nnum_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\nif args.max_train_steps is None:\n    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    overrode_max_train_steps = True\n\nlr_scheduler = get_scheduler(\n    name=args.lr_scheduler_type,\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=args.max_train_steps if overrode_max_train_steps else args.max_train_steps * accelerator.num_processes,\n)\n\n# Accelerator prepare\nmodel, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n)\n\n# ÈáçÊñ∞Ë®àÁÆóÊ≠•Êï∏Ëàá epoch\nnum_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\nif overrode_max_train_steps:\n    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\nargs.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n# --- Metric ---\nmetric = evaluate.load(\"accuracy\")\n\n# Ë®àÁÆóÁ∏Ω batch size\ntotal_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:10:33.826663Z","iopub.execute_input":"2025-09-15T08:10:33.827191Z","iopub.status.idle":"2025-09-15T08:11:25.167637Z","shell.execute_reply.started":"2025-09-15T08:10:33.827169Z","shell.execute_reply":"2025-09-15T08:11:25.166858Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21714 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3434485a7d74fcc8d39349e26eb4234"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3009 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d75f393a54334cb391b4a89636a98a2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0dce39d222244bc9a8d17daaa68790e"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# ===== Training Setup =====\naccelerator.print(\"***** Running training *****\")\naccelerator.print(f\"  Num examples = {len(train_dataset)}\")\naccelerator.print(f\"  Num Epochs = {args.num_train_epochs}\")\naccelerator.print(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\naccelerator.print(f\"  Total train batch size (parallel/distributed/accumulation) = {total_batch_size}\")\naccelerator.print(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\naccelerator.print(f\"  Total optimization steps = {args.max_train_steps}\")\n\nprogress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\ncompleted_steps, starting_epoch = 0, 0\nprogress_bar.update(completed_steps)   # Êõ¥Êñ∞ÈÄ≤Â∫¶Ê¢ùÔºàËã•Âæû checkpoint ÁπºÁ∫åÔºâ\n\n# ===== Training Loop =====\nfor epoch in range(starting_epoch, args.num_train_epochs):\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        if accelerator.sync_gradients:   # ÊØèÂÄã accumulation step ÊâçÊõ¥Êñ∞\n            progress_bar.update(1)\n            completed_steps += 1\n\n        if completed_steps >= args.max_train_steps:\n            break\n\n    # ===== Evaluation =====\n    model.eval()\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n        predictions = outputs.logits.argmax(dim=-1)\n        predictions, references = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n        metric.add_batch(predictions=predictions, references=references)\n\n    eval_metric = metric.compute()\n    accelerator.print(f\"Epoch {epoch}: {eval_metric}\")\n\n# ===== Save Model & Results =====\nif args.output_dir is not None:\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(\n        args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n    )\n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(args.output_dir)\n        with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n            json.dump({f\"eval_{k}\": v for k, v in eval_metric.items()}, f)\n\naccelerator.wait_for_everyone()\naccelerator.end_training()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T08:11:25.168851Z","iopub.execute_input":"2025-09-15T08:11:25.169121Z","iopub.status.idle":"2025-09-15T10:24:55.135255Z","shell.execute_reply.started":"2025-09-15T08:11:25.169095Z","shell.execute_reply":"2025-09-15T10:24:55.134618Z"}},"outputs":[{"name":"stdout","text":"***** Running training *****\n  Num examples = 21714\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (parallel/distributed/accumulation) = 2\n  Gradient Accumulation steps = 2\n  Total optimization steps = 10857\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10857 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e07936072e47f58dc1b322eaf0179d"}},"metadata":{}},{"name":"stderr","text":"You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0: {'accuracy': 0.9521435692921236}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"!zip -r /kaggle/working/working.zip /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T10:31:01.245509Z","iopub.execute_input":"2025-09-15T10:31:01.246108Z","iopub.status.idle":"2025-09-15T10:31:22.212945Z","shell.execute_reply.started":"2025-09-15T10:31:01.246086Z","shell.execute_reply":"2025-09-15T10:31:22.212239Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/all_results.json (stored 0%)\n  adding: kaggle/working/config.json (deflated 54%)\n  adding: kaggle/working/model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 7%)\n  adding: kaggle/working/special_tokens_map.json (deflated 42%)\n  adding: kaggle/working/tokenizer_config.json (deflated 75%)\n  adding: kaggle/working/tokenizer.json (deflated 75%)\n  adding: kaggle/working/vocab.txt (deflated 48%)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/working.zip","text/html":"<a href='/kaggle/working/working.zip' target='_blank'>/kaggle/working/working.zip</a><br>"},"metadata":{}}],"execution_count":19}]}