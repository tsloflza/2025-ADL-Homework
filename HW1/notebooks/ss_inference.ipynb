{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:42.669472Z",
     "iopub.status.busy": "2025-09-16T14:10:42.669140Z",
     "iopub.status.idle": "2025-09-16T14:10:46.399920Z",
     "shell.execute_reply": "2025-09-16T14:10:46.398926Z",
     "shell.execute_reply.started": "2025-09-16T14:10:42.669448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "%pip install datasets numpy torch accelerate pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.402215Z",
     "iopub.status.busy": "2025-09-16T14:10:46.401936Z",
     "iopub.status.idle": "2025-09-16T14:10:46.409707Z",
     "shell.execute_reply": "2025-09-16T14:10:46.408590Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.402190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning a ü§ó Transformers model for question answering using ü§ó Accelerate.\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own question answering task. Pointers for this are left as comments.\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"===== ss_inference.py =====\")\n",
    "\n",
    "# ËÆÄÂèñÂëΩ‰ª§ÂàóÂèÉÊï∏\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--test_file\", type=str, required=True)\n",
    "parser.add_argument(\"--output_file\", type=str, required=True)\n",
    "args_dict = parser.parse_args().__dict__\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    test_file=args_dict[\"test_file\"],\n",
    "    ps_result=\"ps_result.csv\",\n",
    "    max_seq_length=512,\n",
    "    pad_to_max_length=False,\n",
    "    model_name_or_path=\"downloads/ss2_model\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    lr_scheduler_type=SchedulerType.LINEAR, \n",
    "    output_file=args_dict[\"output_file\"],\n",
    "    seed=1234,\n",
    "    doc_stride=128,\n",
    "    n_best_size=20,\n",
    "    max_answer_length=30,\n",
    ")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.410815Z",
     "iopub.status.busy": "2025-09-16T14:10:46.410578Z",
     "iopub.status.idle": "2025-09-16T14:10:46.435788Z",
     "shell.execute_reply": "2025-09-16T14:10:46.434988Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.410794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(\n",
    "    examples,\n",
    "    features,\n",
    "    predictions: tuple[np.ndarray, np.ndarray],\n",
    "    version_2_with_negative: bool = False,\n",
    "    n_best_size: int = 20,\n",
    "    max_answer_length: int = 30,\n",
    "    null_score_diff_threshold: float = 0.0,\n",
    "    output_dir: Optional[str] = None,\n",
    "    prefix: Optional[str] = None,\n",
    "    log_level: Optional[int] = logging.WARNING,\n",
    "):\n",
    "    if len(predictions) != 2:\n",
    "        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n",
    "    all_start_logits, all_end_logits = predictions\n",
    "\n",
    "    if len(predictions[0]) != len(features):\n",
    "        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n",
    "\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_prediction = None\n",
    "        prelim_predictions = []\n",
    "\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
    "            # available in the current feature.\n",
    "            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            feature_null_score = start_logits[0] + end_logits[0]\n",
    "            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n",
    "                min_null_prediction = {\n",
    "                    \"offsets\": (0, 0),\n",
    "                    \"score\": feature_null_score,\n",
    "                    \"start_logit\": start_logits[0],\n",
    "                    \"end_logit\": end_logits[0],\n",
    "                }\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or len(offset_mapping[start_index]) < 2\n",
    "                        or offset_mapping[end_index] is None\n",
    "                        or len(offset_mapping[end_index]) < 2\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
    "                    # provided).\n",
    "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
    "                        continue\n",
    "\n",
    "                    prelim_predictions.append(\n",
    "                        {\n",
    "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"start_logit\": start_logits[start_index],\n",
    "                            \"end_logit\": end_logits[end_index],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Only keep the best `n_best_size` predictions.\n",
    "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "\n",
    "        # Use the offsets to gather the answer text in the original context.\n",
    "        context = example[\"context\"]\n",
    "        for pred in predictions:\n",
    "            offsets = pred.pop(\"offsets\")\n",
    "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
    "\n",
    "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "        # failure.\n",
    "        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n",
    "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
    "\n",
    "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
    "        # the LogSumExp trick).\n",
    "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "        # Include the probabilities in our predictions.\n",
    "        for prob, pred in zip(probs, predictions):\n",
    "            pred[\"probability\"] = prob\n",
    "\n",
    "        # Pick the best prediction. If the null answer is not possible, this is easy.\n",
    "        all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
    "\n",
    "        # Make `predictions` JSON-serializable by casting np.float back to float.\n",
    "        all_nbest_json[example[\"id\"]] = [\n",
    "            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "    # If we have an output_dir, let's save all those dicts.\n",
    "    if output_dir is not None:\n",
    "        if not os.path.isdir(output_dir):\n",
    "            raise OSError(f\"{output_dir} is not a directory.\")\n",
    "\n",
    "        prediction_file = os.path.join(\n",
    "            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n",
    "        )\n",
    "        nbest_file = os.path.join(\n",
    "            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n",
    "        )\n",
    "\n",
    "        print(f\"Saving predictions to {prediction_file}.\")\n",
    "        with open(prediction_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "        print(f\"Saving nbest_preds to {nbest_file}.\")\n",
    "        with open(nbest_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.438044Z",
     "iopub.status.busy": "2025-09-16T14:10:46.437830Z",
     "iopub.status.idle": "2025-09-16T14:10:46.455188Z",
     "shell.execute_reply": "2025-09-16T14:10:46.454321Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.438029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_prefixed_metrics(results, output_dir, file_name: str = \"all_results.json\", metric_key_prefix: str = \"eval\"):\n",
    "    \"\"\"\n",
    "    Save results while prefixing metric names.\n",
    "\n",
    "    Args:\n",
    "        results: (:obj:`dict`):\n",
    "            A dictionary of results.\n",
    "        output_dir: (:obj:`str`):\n",
    "            An output directory.\n",
    "        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\n",
    "            An output file name.\n",
    "        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\n",
    "            A metric name prefix.\n",
    "    \"\"\"\n",
    "    # Prefix all keys with metric_key_prefix + '_'\n",
    "    for key in list(results.keys()):\n",
    "        if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "            results[f\"{metric_key_prefix}_{key}\"] = results.pop(key)\n",
    "\n",
    "    with open(os.path.join(output_dir, file_name), \"w\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.474980Z",
     "iopub.status.busy": "2025-09-16T14:10:46.474696Z",
     "iopub.status.idle": "2025-09-16T14:10:46.492003Z",
     "shell.execute_reply": "2025-09-16T14:10:46.491306Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.474955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Accelerator =====\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.493231Z",
     "iopub.status.busy": "2025-09-16T14:10:46.492944Z",
     "iopub.status.idle": "2025-09-16T14:10:47.573071Z",
     "shell.execute_reply": "2025-09-16T14:10:47.572298Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.493202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# === ËºâÂÖ• PS ÁµêÊûú (paragraph selection È†êÊ∏¨) ===\n",
    "ps_df = pd.read_csv(args.ps_result)\n",
    "\n",
    "# === ËºâÂÖ•Ê∏¨Ë©¶Ê™î (ÂïèÈ°åËàá id) ===\n",
    "with open(args.test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# === ÊßãÂª∫ span selection inference dataset ===\n",
    "examples = []\n",
    "for ex in test_data:\n",
    "    qid = ex[\"id\"]\n",
    "    question = ex[\"question\"]\n",
    "\n",
    "    # Âæû ps_result ÊâæÂá∫Â∞çÊáâÁöÑ paragraph (Â∑≤Á∂ìÊòØÊñáÂ≠ó)\n",
    "    pred_para = ps_df.loc[ps_df[\"id\"] == qid, \"prediction\"].values[0]\n",
    "\n",
    "    examples.append({\n",
    "        \"id\": qid,\n",
    "        \"question\": question,\n",
    "        \"context\": pred_para,\n",
    "    })\n",
    "\n",
    "# === Âª∫Á´ã Dataset ===\n",
    "raw_datasets = datasets.DatasetDict({\n",
    "    \"test\": datasets.Dataset.from_list(examples)\n",
    "})\n",
    "\n",
    "print(raw_datasets)\n",
    "print(raw_datasets[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:47.574481Z",
     "iopub.status.busy": "2025-09-16T14:10:47.574040Z",
     "iopub.status.idle": "2025-09-16T14:10:47.745006Z",
     "shell.execute_reply": "2025-09-16T14:10:47.744022Z",
     "shell.execute_reply.started": "2025-09-16T14:10:47.574450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========= Load pretrained model and tokenizer =========\n",
    "config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    ").to(device)\n",
    "print(model.device)\n",
    "\n",
    "# ========= Dataset column names =========\n",
    "column_names = raw_datasets[\"test\"].column_names\n",
    "question_column_name = \"question\"\n",
    "context_column_name = \"context\"\n",
    "\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:47.746944Z",
     "iopub.status.busy": "2025-09-16T14:10:47.746210Z",
     "iopub.status.idle": "2025-09-16T14:10:50.406350Z",
     "shell.execute_reply": "2025-09-16T14:10:50.405650Z",
     "shell.execute_reply.started": "2025-09-16T14:10:47.746920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ========= Preprocessing (Inference) =========\n",
    "def prepare_validation_features(examples):\n",
    "    # ÂéªÈô§ÈñãÈ†≠Á©∫ÁôΩ\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # tokenize question-context pair\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,   # ‰øùÁïô offset_mapping\n",
    "        padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Âè™‰øùÁïô context ÈÉ®ÂàÜÁöÑ offset_mapping\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "test_examples = raw_datasets[\"test\"]\n",
    "test_dataset = test_examples.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Tokenizing test dataset\",\n",
    ")\n",
    "\n",
    "# Âª∫Á´ã dataset Áµ¶ modelÔºàÂéªÊéâ offset_mappingÔºâ\n",
    "test_dataset_for_model = test_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "\n",
    "# ========= DataLoader =========\n",
    "if args.pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset_for_model,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=args.per_device_eval_batch_size,\n",
    ")\n",
    "\n",
    "print(test_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:50.408659Z",
     "iopub.status.busy": "2025-09-16T14:10:50.408405Z",
     "iopub.status.idle": "2025-09-16T14:29:36.582838Z",
     "shell.execute_reply": "2025-09-16T14:29:36.582032Z",
     "shell.execute_reply.started": "2025-09-16T14:10:50.408640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Inference =====\n",
    "print(\"***** Running Inference *****\")\n",
    "print(f\"  Num examples = {len(test_dataset)}\")\n",
    "print(f\"  Batch size = {args.per_device_eval_batch_size}\")\n",
    "\n",
    "all_start_logits = []\n",
    "all_end_logits = []\n",
    "model.eval()\n",
    "\n",
    "for step, batch in enumerate(tqdm(test_dataloader, desc=\"Inference\", disable=not accelerator.is_local_main_process)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.to(model.device) for k, v in batch.items()})\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "        if not args.pad_to_max_length:\n",
    "            start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "            end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "        all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n",
    "        all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n",
    "\n",
    "\n",
    "# ===== Êï¥ÁêÜ logits =====\n",
    "\n",
    "# Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
    "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n",
    "    step = 0\n",
    "    # create a numpy array and fill it with -100.\n",
    "    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n",
    "    # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather_for_metrics\n",
    "    for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n",
    "        # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n",
    "        # And after every iteration we have to change the step\n",
    "\n",
    "        batch_size = output_logit.shape[0]\n",
    "        cols = output_logit.shape[1]\n",
    "\n",
    "        if step + batch_size < len(dataset):\n",
    "            logits_concat[step : step + batch_size, :cols] = output_logit\n",
    "        else:\n",
    "            logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n",
    "\n",
    "        step += batch_size\n",
    "\n",
    "    return logits_concat\n",
    "\n",
    "max_len = max(x.shape[1] for x in all_start_logits)\n",
    "start_logits_concat = create_and_fill_np_array(all_start_logits, test_dataset, max_len)\n",
    "end_logits_concat = create_and_fill_np_array(all_end_logits, test_dataset, max_len)\n",
    "\n",
    "# Ê∏ÖÁêÜÊö´Â≠ò\n",
    "del all_start_logits\n",
    "del all_end_logits\n",
    "\n",
    "# ===== ÂæåËôïÁêÜÔºåËΩâÊàêÊñáÂ≠óÁ≠îÊ°à =====\n",
    "outputs_numpy = (start_logits_concat, end_logits_concat)\n",
    "predictions = postprocess_qa_predictions(\n",
    "    examples=test_examples,\n",
    "    features=test_dataset,\n",
    "    predictions=outputs_numpy,\n",
    "    version_2_with_negative=False,\n",
    "    n_best_size=args.n_best_size,\n",
    "    max_answer_length=args.max_answer_length,\n",
    "    null_score_diff_threshold=0.0,\n",
    "    output_dir=None,   # ‰∏çÁî®Ëº∏Âá∫Âà∞‰∏≠ÈñìÊ™î\n",
    "    prefix=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:29:36.584367Z",
     "iopub.status.busy": "2025-09-16T14:29:36.583754Z",
     "iopub.status.idle": "2025-09-16T14:29:36.594578Z",
     "shell.execute_reply": "2025-09-16T14:29:36.593697Z",
     "shell.execute_reply.started": "2025-09-16T14:29:36.584339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Â≠òÊàê CSV =====\n",
    "if accelerator.is_main_process:\n",
    "    with open(args.output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"answer\"])\n",
    "        for qid, ans in predictions.items():\n",
    "            writer.writerow([qid, ans])\n",
    "\n",
    "    print(f\"‚úÖ Saved predictions to {args.output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13653249,
     "sourceId": 114453,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
