{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:42.669472Z",
     "iopub.status.busy": "2025-09-16T14:10:42.669140Z",
     "iopub.status.idle": "2025-09-16T14:10:46.399920Z",
     "shell.execute_reply": "2025-09-16T14:10:46.398926Z",
     "shell.execute_reply.started": "2025-09-16T14:10:42.669448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 19 00:43:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.76.05              Driver Version: 580.76.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   0  NVIDIA RTX A6000               On  |   00000000:3D:00.0 Off |                  Off |\n",
      "| 30%   39C    P8             21W /  300W |       4MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  |   00000000:3E:00.0 Off |                  Off |\n",
      "| 64%   87C    P2            291W /  300W |   41520MiB /  49140MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               On  |   00000000:61:00.0 Off |                  Off |\n",
      "| 44%   69C    P2            283W /  300W |   47078MiB /  49140MiB |     96%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               On  |   00000000:B1:00.0 Off |                  Off |\n",
      "| 30%   56C    P2            255W /  300W |   41536MiB /  49140MiB |    100%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000               On  |   00000000:B2:00.0 Off |                  Off |\n",
      "| 30%   57C    P2            194W /  300W |   32242MiB /  49140MiB |     79%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000               On  |   00000000:DA:00.0 Off |                  Off |\n",
      "| 30%   42C    P2            131W /  300W |    6610MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA RTX A6000               On  |   00000000:DB:00.0 Off |                  Off |\n",
      "| 30%   51C    P2            266W /  300W |    6636MiB /  49140MiB |     96%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    1   N/A  N/A         1356361      C   python3                               41488MiB |\n",
      "|    2   N/A  N/A          678186      C   python3                               47044MiB |\n",
      "|    3   N/A  N/A          495054      C   python3                               41502MiB |\n",
      "|    4   N/A  N/A         3367446      C   python                                32232MiB |\n",
      "|    5   N/A  N/A         1156724      C   python3                                6600MiB |\n",
      "|    6   N/A  N/A         1157607      C   python3                                6626MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: evaluate in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (0.4.5)\n",
      "Requirement already satisfied: numpy in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: torch in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: accelerate in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: pandas in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: tqdm in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (0.35.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.9.0,>=2023.1.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (2025.9.0)\n",
      "Requirement already satisfied: packaging in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: filelock in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: xxhash in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: networkx in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: jinja2 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from triton==3.4.0->torch) (65.5.0)\n",
      "Requirement already satisfied: psutil in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: six>=1.5 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tsloflza/.pyenv/versions/3.10.18/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "%pip install datasets evaluate numpy torch accelerate pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.402215Z",
     "iopub.status.busy": "2025-09-16T14:10:46.401936Z",
     "iopub.status.idle": "2025-09-16T14:10:46.409707Z",
     "shell.execute_reply": "2025-09-16T14:10:46.408590Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.402190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning a ğŸ¤— Transformers model for question answering using ğŸ¤— Accelerate.\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own question answering task. Pointers for this are left as comments.\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    SchedulerType,\n",
    "    default_data_collator,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.456402Z",
     "iopub.status.busy": "2025-09-16T14:10:46.456096Z",
     "iopub.status.idle": "2025-09-16T14:10:46.473889Z",
     "shell.execute_reply": "2025-09-16T14:10:46.473161Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.456377Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(test_file='ntu-adl-2025-hw-1/test.json', context_file='ntu-adl-2025-hw-1/context.json', max_seq_length=512, pad_to_max_length=False, model_name_or_path='output', per_device_eval_batch_size=1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, output_path='output/prediction.csv', seed=1234, doc_stride=128, n_best_size=20, max_answer_length=30)\n"
     ]
    }
   ],
   "source": [
    "args = SimpleNamespace(\n",
    "    test_file=\"ntu-adl-2025-hw-1/test.json\",\n",
    "    context_file=\"ntu-adl-2025-hw-1/context.json\",\n",
    "    max_seq_length=512,\n",
    "    pad_to_max_length=False,\n",
    "    model_name_or_path=\"output\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    lr_scheduler_type=SchedulerType.LINEAR, \n",
    "    output_path=\"output/prediction.csv\",\n",
    "    seed=1234,\n",
    "    doc_stride=128,\n",
    "    n_best_size=20,\n",
    "    max_answer_length=30,\n",
    ")\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.410815Z",
     "iopub.status.busy": "2025-09-16T14:10:46.410578Z",
     "iopub.status.idle": "2025-09-16T14:10:46.435788Z",
     "shell.execute_reply": "2025-09-16T14:10:46.434988Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.410794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def postprocess_qa_predictions(\n",
    "    examples,\n",
    "    features,\n",
    "    predictions: tuple[np.ndarray, np.ndarray],\n",
    "    version_2_with_negative: bool = False,\n",
    "    n_best_size: int = 20,\n",
    "    max_answer_length: int = 30,\n",
    "    null_score_diff_threshold: float = 0.0,\n",
    "    output_dir: Optional[str] = None,\n",
    "    prefix: Optional[str] = None,\n",
    "    log_level: Optional[int] = logging.WARNING,\n",
    "):\n",
    "    if len(predictions) != 2:\n",
    "        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n",
    "    all_start_logits, all_end_logits = predictions\n",
    "\n",
    "    if len(predictions[0]) != len(features):\n",
    "        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n",
    "\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "\n",
    "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_prediction = None\n",
    "        prelim_predictions = []\n",
    "\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
    "            # available in the current feature.\n",
    "            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            feature_null_score = start_logits[0] + end_logits[0]\n",
    "            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n",
    "                min_null_prediction = {\n",
    "                    \"offsets\": (0, 0),\n",
    "                    \"score\": feature_null_score,\n",
    "                    \"start_logit\": start_logits[0],\n",
    "                    \"end_logit\": end_logits[0],\n",
    "                }\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or len(offset_mapping[start_index]) < 2\n",
    "                        or offset_mapping[end_index] is None\n",
    "                        or len(offset_mapping[end_index]) < 2\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
    "                    # provided).\n",
    "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
    "                        continue\n",
    "\n",
    "                    prelim_predictions.append(\n",
    "                        {\n",
    "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"start_logit\": start_logits[start_index],\n",
    "                            \"end_logit\": end_logits[end_index],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Only keep the best `n_best_size` predictions.\n",
    "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "\n",
    "        # Use the offsets to gather the answer text in the original context.\n",
    "        context = example[\"context\"]\n",
    "        for pred in predictions:\n",
    "            offsets = pred.pop(\"offsets\")\n",
    "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
    "\n",
    "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "        # failure.\n",
    "        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n",
    "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
    "\n",
    "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
    "        # the LogSumExp trick).\n",
    "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "        # Include the probabilities in our predictions.\n",
    "        for prob, pred in zip(probs, predictions):\n",
    "            pred[\"probability\"] = prob\n",
    "\n",
    "        # Pick the best prediction. If the null answer is not possible, this is easy.\n",
    "        all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
    "\n",
    "        # Make `predictions` JSON-serializable by casting np.float back to float.\n",
    "        all_nbest_json[example[\"id\"]] = [\n",
    "            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "    # If we have an output_dir, let's save all those dicts.\n",
    "    if output_dir is not None:\n",
    "        if not os.path.isdir(output_dir):\n",
    "            raise OSError(f\"{output_dir} is not a directory.\")\n",
    "\n",
    "        prediction_file = os.path.join(\n",
    "            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n",
    "        )\n",
    "        nbest_file = os.path.join(\n",
    "            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n",
    "        )\n",
    "\n",
    "        print(f\"Saving predictions to {prediction_file}.\")\n",
    "        with open(prediction_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "        print(f\"Saving nbest_preds to {nbest_file}.\")\n",
    "        with open(nbest_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.438044Z",
     "iopub.status.busy": "2025-09-16T14:10:46.437830Z",
     "iopub.status.idle": "2025-09-16T14:10:46.455188Z",
     "shell.execute_reply": "2025-09-16T14:10:46.454321Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.438029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_prefixed_metrics(results, output_dir, file_name: str = \"all_results.json\", metric_key_prefix: str = \"eval\"):\n",
    "    \"\"\"\n",
    "    Save results while prefixing metric names.\n",
    "\n",
    "    Args:\n",
    "        results: (:obj:`dict`):\n",
    "            A dictionary of results.\n",
    "        output_dir: (:obj:`str`):\n",
    "            An output directory.\n",
    "        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\n",
    "            An output file name.\n",
    "        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\n",
    "            A metric name prefix.\n",
    "    \"\"\"\n",
    "    # Prefix all keys with metric_key_prefix + '_'\n",
    "    for key in list(results.keys()):\n",
    "        if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "            results[f\"{metric_key_prefix}_{key}\"] = results.pop(key)\n",
    "\n",
    "    with open(os.path.join(output_dir, file_name), \"w\") as f:\n",
    "        json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.474980Z",
     "iopub.status.busy": "2025-09-16T14:10:46.474696Z",
     "iopub.status.idle": "2025-09-16T14:10:46.492003Z",
     "shell.execute_reply": "2025-09-16T14:10:46.491306Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.474955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== Accelerator =====\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:46.493231Z",
     "iopub.status.busy": "2025-09-16T14:10:46.492944Z",
     "iopub.status.idle": "2025-09-16T14:10:47.573071Z",
     "shell.execute_reply": "2025-09-16T14:10:47.572298Z",
     "shell.execute_reply.started": "2025-09-16T14:10:46.493202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['id', 'question', 'context'],\n",
      "        num_rows: 2213\n",
      "    })\n",
      "})\n",
      "{'id': '5e7a923dd6e4ccb8730eb95230e0c908', 'question': 'å¡åˆ©å‰µç«‹çš„ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨è¦åˆ°ä»€éº¼æ™‚å¾Œæ‰é–‹æ”¾å­˜å–ï¼Ÿ', 'context': 'ç¡¬é«”æ˜¯è£ç½®åœ¨æ©Ÿç®±å…§ä»¥åšå‡ºå€‹äººé›»è…¦ã€‚ç³»çµ±è»Ÿé«”æ˜¯å„²å­˜åœ¨ç¡¬é«”å…§ï¼Œè€Œç³»çµ±è»Ÿé«”å…§å«æœ‰éŸŒé«”ï¼Œä¾‹å¦‚åŸºæœ¬è¼¸å…¥è¼¸å‡ºç³»çµ±ä»¥åŠä½œæ¥­ç³»çµ±ï¼Œé€™äº›è»Ÿé«”ä½¿æ‡‰ç”¨è»Ÿé«”å¯ä»¥æä¾›ä½¿ç”¨è€…æ‰€éœ€çš„åŠŸèƒ½ã€‚ä½œæ¥­ç³»çµ±é€šå¸¸è—‰ç”±åŒ¯æµæ’èˆ‡è£ç½®æºé€šï¼Œé€™éœ€è¦è»Ÿé«”æä¾›é©…å‹•ç¨‹å¼ã€‚è¼¸å…¥å’Œè¼¸å‡ºè£ç½®é€šå¸¸è£åœ¨å¤–éƒ¨çš„ä¸»è¨ˆç®—æ©Ÿæ©Ÿç®±ã€‚è¼¸å…¥è£ç½®ä½¿ç”¨æˆ¶èƒ½å¤ å°‡è³‡è¨Šè¼¸å…¥åˆ°ç³»çµ±ä¸­ï¼Œæˆ–æ§åˆ¶å…¶æ“ä½œã€‚å¤§å¤šæ•¸å€‹äººè¨ˆç®—æ©Ÿçš„é¼ æ¨™å’Œéµç›¤ï¼Œä½†ç­†è¨˜æœ¬é›»è…¦çš„ç³»çµ±é€šå¸¸ä½¿ç”¨çš„é¼ æ¨™è§¸æ‘¸æ¿ä¾†ä»£æ›¿ã€‚å…¶ä»–çš„è¼¸å…¥è£ç½®åŒ…æ‹¬ç¶²è·¯æ”åƒé ­ï¼Œéº¥å…‹é¢¨ï¼ŒéŠæˆ²æ¡¿å’Œåœ–åƒæƒæå„€ã€‚è¼¸å‡ºè£ç½®åœ¨äººé¡å¯è®€çš„å½¢å¼é¡¯ç¤ºè³‡è¨Šã€‚é€™ç¨®è£ç½®å¯ä»¥åŒ…æ‹¬å°è¡¨æ©Ÿï¼ŒéŸ³ç®±ï¼Œé¡¯ç¤ºå™¨æˆ–ç›²æ–‡å£“èŠ±ã€‚è¨ˆç®—æ©Ÿè³‡æ–™å­˜å„²ï¼Œé€šå¸¸ç¨±ç‚ºå­˜å„²æˆ–å…§å­˜ï¼Œæ˜¯æŒ‡è¨ˆç®—æ©Ÿéƒ¨ä»¶å’Œè¨˜éŒ„åª’ä»‹çš„ä¿ç•™æ•¸å­—è³‡æ–™ã€‚è³‡æ–™å­˜å„²æ˜¯ä¸€å€‹æ ¸å¿ƒåŠŸèƒ½å’Œè¨ˆç®—æ©Ÿçš„åŸºæœ¬çµ„æˆéƒ¨åˆ†ã€‚ 1996å¹´ï¼Œå¡åˆ©å‰µç«‹äº†ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨ï¼›åŒå¹´10æœˆé–‹å§‹æ”¶é›†å„²å­˜è³‡æ–™ã€‚ä¸éï¼Œç›´åˆ°2001å¹´é–‹ç™¼äº†ã€Œæ™‚å…‰æ©Ÿã€å‰ï¼Œé€™äº›è³‡æ–™éƒ½ç„¡æ³•å­˜å–ã€‚1999å¹´æœ«æ“´å±•æ”¶é›†ç¯„åœã€‚2012å¹´8æœˆï¼Œå®£å‚³å°‡åœ¨å…¶ç¾å­˜çš„130è¬æª”æ¡ˆçš„ä¸‹è¼‰é¸é …ä¸­åŠ å…¥ä½å…ƒæ´ªæµã€‚å› ç‚ºé€šéå…©å€‹æª”æ¡ˆè³‡æ–™ä¸­å¿ƒå”èª¿ï¼Œé€™æˆç‚ºå¾è©²æª”æ¡ˆé¤¨ä¸‹è¼‰è³‡æ–™çš„æœ€å¿«æ–¹æ³•ã€‚2013å¹´11æœˆ6æ—¥ï¼Œæª”æ¡ˆé¤¨åœ¨é‡Œå¥‡è’™å¾·å€çš„ç¸½éƒ¨å¤±ç«ï¼Œæå£äº†è¨±å¤šè£ç½®å’Œä¸€äº›é™„è¿‘çš„å…¬å¯“ï¼Œé è¨ˆæå¤±é”åˆ°60è¬ç¾å…ƒã€‚æ™‚å…‰æ©Ÿæ˜¯ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨æœ€é‡è¦çš„æœå‹™ä¹‹ä¸€ã€‚æ™‚å…‰æ©Ÿå…è¨±äººå€‘å»æœå°‹å’Œå­˜å–å…¶ç¶²é å­˜æª”ã€‚åœ¨ä¸€äº›åœ‹å®¶å’Œåœ°å€ï¼Œæ™‚å…‰æ©Ÿé€™å€‹è¡“èªçš„ä½¿ç”¨å·²ç¶“éå¸¸æ™®éï¼Œã€Œæ™‚å…‰æ©Ÿã€å’Œã€Œç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨ã€ç”šè‡³é–‹å§‹è¢«ç•¶åšåŒç¾©è©ä½¿ç”¨ã€‚ 1994å¹´10æœˆï¼Œå…¨çƒè³‡è¨Šç¶²å”æœƒåœ¨éº»çœç†å·¥å­¸é™¢é›»è…¦ç§‘å­¸å¯¦é©—å®¤æˆç«‹ï¼Œå»ºç«‹è€…æ˜¯å…¨çƒè³‡è¨Šç¶²çš„ç™¼æ˜è€…æå§†Â·æŸå…§èŒ²-æã€‚åˆ°äº†1994å¹´åº•ï¼Œå…¨çƒç¶²ç«™æ•¸é‡ä¾ç„¶ç›¸å°è¼ƒå°‘ï¼Œä½†æ˜¯å¾ˆå¤šè‘—åç¶²ç«™å·²ç¶“ç›¸ç•¶æ´»èºï¼Œé€™äº›ç¶²ç«™å·²ç¶“é ç¤ºæˆ–è€…å•Ÿç™¼äº†ç•¶ä»Šæœ€æµè¡Œçš„æœå‹™ã€‚é€éç¶²éš›ç¶²è·¯é€£æ¥ï¼Œä¸–ç•Œå„åœ°ä¹Ÿå»ºç«‹äº†å…¶ä»–ç¶²ç«™ã€‚é€™ä¿ƒé€²äº†å”è­°å’Œæ ¼å¼åŒ–çš„åœ‹éš›æ¨™æº–ç™¼å±•ã€‚æŸå…§èŒ²-æç¹¼çºŒåƒèˆ‡æŒ‡å°å…¨çƒè³‡è¨Šç¶²æ¨™æº–çš„ç™¼å±•ï¼Œä¾‹å¦‚æ¨™è¨˜å¼èªè¨€ä¾†çµ„æˆç¶²é å’Œä»–ä¸»å¼µçš„èªç¾©ç¶²é¡˜æ™¯ã€‚å…¨çƒè³‡è¨Šç¶²é€éæ˜“æ–¼ä½¿ç”¨å’Œéˆæ´»çš„æ ¼å¼åœ¨ç¶²éš›ç¶²è·¯ä¸Šå‚³æ’­è³‡è¨Šï¼Œå› æ­¤å°æ–¼ç¶²éš›ç¶²è·¯çš„æ™®åŠç™¼æ®äº†é‡è¦çš„ä½œç”¨ã€‚é›–ç„¶é€™å…©å€‹è¡“èªæœ‰æ™‚è¢«äººå€‘å»£æ³›ä½¿ç”¨ï¼Œä½†å…¨çƒè³‡è¨Šç¶²ä¸¦ä¸æ˜¯ç¶²éš›ç¶²è·¯çš„ä»£åè©ã€‚å…¨çƒè³‡è¨Šç¶²æ˜¯ä¸€å€‹è³‡è¨Šç©ºé–“ï¼ŒåŒ…å«è¶…é€£çµæ–‡ä»¶å’Œå…¶ä»–è³‡æºï¼Œä¸¦ä¸”ç”±å®ƒå€‘çš„URIsæ¨™è­˜ã€‚ç¶²éš›ç¶²è·¯å’Œå…¨çƒè³‡è¨Šç¶²é€™å…©å€‹è©é€šå¸¸æ²’æœ‰å¤šå°‘å€åˆ¥ã€‚ä½†æ˜¯ï¼Œå…©è€…ä¸¦ä¸ç›¸åŒã€‚ç¶²éš›ç¶²è·¯æ˜¯ä¸€å€‹å…¨çƒäº’ç›¸é€£æ¥çš„é›»è…¦ç¶²è·¯ç³»çµ±ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¨çƒè³‡è¨Šç¶²æ˜¯é€éè¶…é€£çµå’ŒURIsé€£æ¥çš„å…¨çƒæ”¶é›†æª”æ¡ˆå’Œå…¶ä»–è³‡æºã€‚å…¨çƒè³‡è¨Šç¶²è³‡æºé€šå¸¸ä½¿ç”¨HTTPå­˜å–ï¼Œé€™æ˜¯è¨±å¤šç¶²éš›ç¶²è·¯é€šè¨Šå”è­°çš„å…¶ä¸­ä¹‹ä¸€ã€‚è‹¥è¦é€²å…¥å…¨çƒè³‡è¨Šç¶²ä¸Šä¸€å€‹ç¶²é ï¼Œæˆ–è€…å…¶ä»–ç¶²è·¯è³‡æºçš„æ™‚å€™ï¼Œé€šå¸¸éœ€ç€è¦½å™¨ä¸Šéµå…¥ä½ æƒ³å­˜å–ç¶²é çš„çµ±ä¸€è³‡æºå®šä½ç¬¦ï¼Œæˆ–è€…é€šéè¶…é€£çµæ–¹å¼é€£çµåˆ°é‚£å€‹ç¶²é æˆ–ç¶²è·¯è³‡æºã€‚é€™ä¹‹å¾Œçš„å·¥ä½œé¦–å…ˆæ˜¯URLçš„ä¼ºæœå™¨åéƒ¨åˆ†ï¼Œè¢«åç‚ºåŸŸåç³»çµ±çš„åˆ†å¸ƒæ–¼å…¨çƒçš„ç¶²éš›ç¶²è·¯è³‡æ–™åº«è§£æï¼Œä¸¦æ ¹æ“šè§£æçµæœæ±ºå®šé€²å…¥å“ªä¸€å€‹IPä½å€ã€‚æ¥ä¸‹ä¾†çš„æ­¥é©Ÿæ˜¯ç‚ºæ‰€è¦å­˜å–çš„ç¶²é ï¼Œå‘åœ¨é‚£å€‹IPä½å€å·¥ä½œçš„ä¼ºæœå™¨å‚³é€ä¸€å€‹HTTPè«‹æ±‚ã€‚åœ¨é€šå¸¸æƒ…æ³ä¸‹ï¼ŒHTMLæ–‡å­—ã€åœ–ç‰‡å’Œæ§‹æˆè©²ç¶²é çš„ä¸€åˆ‡å…¶ä»–æª”æ¡ˆå¾ˆå¿«æœƒè¢«é€ä¸€è«‹æ±‚ä¸¦è¡Œé€å›ç”¨æˆ¶ã€‚ç¶²è·¯ç€è¦½å™¨æ¥ä¸‹ä¾†çš„å·¥ä½œæ˜¯æŠŠHTMLã€CSSå’Œå…¶ä»–æ¥å—åˆ°çš„æª”æ¡ˆæ‰€æè¿°çš„å…§å®¹ï¼ŒåŠ ä¸Šåœ–åƒã€é€£çµå’Œå…¶ä»–å¿…é ˆçš„è³‡æºï¼Œé¡¯ç¤ºçµ¦ç”¨æˆ¶ã€‚é€™äº›å°±æ§‹æˆäº†ä½ æ‰€çœ‹åˆ°çš„ç¶²é ã€‚å¤§å¤šæ•¸çš„ç¶²é è‡ªèº«åŒ…å«æœ‰è¶…é€£çµæŒ‡å‘å…¶ä»–ç›¸é—œç¶²é ï¼Œå¯èƒ½é‚„æœ‰ä¸‹è¼‰ã€æºæ–‡ç»ã€å®šç¾©å’Œå…¶ä»–ç¶²è·¯è³‡æºã€‚åƒé€™æ¨£é€šéè¶…é€£çµï¼ŒæŠŠæœ‰ç”¨çš„ç›¸é—œè³‡æºçµ„ç¹”åœ¨ä¸€èµ·çš„é›†åˆï¼Œå°±å½¢æˆäº†ä¸€å€‹æ‰€è¬‚çš„è³‡è¨Šçš„ç¶²ã€‚é€™å€‹ç¶²åœ¨ç¶²éš›ç¶²è·¯ä¸Šè¢«æ–¹ä¾¿ä½¿ç”¨ï¼Œå°±æ§‹æˆäº†æœ€æ—©åœ¨1990å¹´ä»£åˆæå§†Â·æŸå…§èŒ²-ææ‰€èªªçš„å…¨çƒè³‡è¨Šç¶²ã€‚ ç¾ä»£æ™‚æœŸçš„é–‹å§‹ä»£è¡¨äº†æ­æ´²æ–‡è—å¾©èˆˆçš„çµæŸã€‚ç¢ºåˆ‡çš„æ™‚é–“é»æœƒä¾æ“šå„å€‹é ˜åŸŸçš„ç•Œå®šè€Œæœ‰æ‰€ä¸åŒï¼Œæ¯”å¦‚èªªï¼Œä¸€å€‹æ­·å²å­¸å®¶å¯èƒ½æœƒå°‡ç¾ä»£çš„é–‹å§‹æ”¾åœ¨1650å¹´ï¼Œè€Œä¸€å€‹éŸ³æ¨‚å®¶å‰‡å¯èƒ½æœƒæŠŠæ™‚é–“æ”¾åœ¨éŸ³æ¨‚çš„æµªæ¼«æ™‚æœŸçµæŸä¹‹å¾Œï¼Œä¹Ÿå°±æ˜¯å¤§ç´„1900å¹´ã€‚ç•¶ä»£å²å­¸è€…ç¿’æ…£æŠŠä¸€æˆ°çˆ†ç™¼ï¼Œä½œç‚ºè¿‘ä»£çš„çµ‚çµã€ç¾ä»£çš„é–‹ç«¯ã€‚åŒæ¨£åœ°ï¼Œä¾æ“šå„å€‹é ˜åŸŸçš„ä¸åŒè§€é»ï¼Œç¾ä»£æ™‚æœŸå¯èƒ½æœƒè¢«ç•Œå®šç‚ºä¸€ç›´å»¶ä¼¸åˆ°æˆ‘å€‘ç¾åœ¨æ‰€è™•çš„æ™‚é–“ï¼Œæˆ–è€…ä¹Ÿæœ‰å¯èƒ½è¢«èªç‚ºçµæŸæ–¼å¾Œç¾ä»£ä¸»ç¾©çš„é–‹å§‹ï¼Œå¯èƒ½åœ¨1960å¹´ä»£åˆ°1980å¹´ä»£åˆæœŸä¹‹é–“çš„ä»»ä½•ä¸€é»ã€‚å¦‚æœæ˜¯å°‡ç¾ä»£å®šç¾©ç‚ºåœ¨å¾Œç¾ä»£ä¹‹å‰çš„è©±ï¼Œé‚£éº¼å®ƒå¯èƒ½ç‰¹åˆ¥æŒ‡æ¶‰äº†ç¾ä»£ä¸»ç¾©ã€‚æœ‰çš„è§€é»å‰‡èªç‚ºï¼Œå¾Œç¾ä»£ä¸»ç¾©åªä¸éæ˜¯ç¾ä»£ä¸»ç¾©æœ¬èº«æœ€æ™šä¸€å€‹æ™‚æœŸçš„ç™¼å±•è€Œå·²ã€‚ '}\n"
     ]
    }
   ],
   "source": [
    "# === è¼‰å…¥ context æ®µè½ ===\n",
    "with open(args.context_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    contexts = json.load(f)\n",
    "\n",
    "# === è¼‰å…¥ test.json ===\n",
    "with open(args.test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# === æ§‹å»º joint inference dataset ===\n",
    "examples = []\n",
    "for ex in test_data:\n",
    "    qid = ex[\"id\"]\n",
    "    question = ex[\"question\"]\n",
    "    paragraphs = ex[\"paragraphs\"]   # å››å€‹å€™é¸æ®µè½ id\n",
    "\n",
    "    # æ‹¼æ¥æ®µè½\n",
    "    concatenated_context = \"\"\n",
    "    for pid in paragraphs:\n",
    "        concatenated_context += contexts[pid] + \" \"   # ä¿æŒå’Œè¨“ç·´ä¸€è‡´ï¼ŒåŠ ç©ºæ ¼\n",
    "\n",
    "    examples.append({\n",
    "        \"id\": qid,\n",
    "        \"question\": question,\n",
    "        \"context\": concatenated_context,\n",
    "    })\n",
    "\n",
    "# === å»ºç«‹ Dataset ===\n",
    "raw_datasets = datasets.DatasetDict({\n",
    "    \"test\": datasets.Dataset.from_list(examples)\n",
    "})\n",
    "\n",
    "print(raw_datasets)\n",
    "print(raw_datasets[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:47.574481Z",
     "iopub.status.busy": "2025-09-16T14:10:47.574040Z",
     "iopub.status.idle": "2025-09-16T14:10:47.745006Z",
     "shell.execute_reply": "2025-09-16T14:10:47.744022Z",
     "shell.execute_reply.started": "2025-09-16T14:10:47.574450Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ========= Load pretrained model and tokenizer =========\n",
    "config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "    config=config,\n",
    ").to(device)\n",
    "print(model.device)\n",
    "\n",
    "# ========= Dataset column names =========\n",
    "column_names = raw_datasets[\"test\"].column_names\n",
    "question_column_name = \"question\"\n",
    "context_column_name = \"context\"\n",
    "\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:47.746944Z",
     "iopub.status.busy": "2025-09-16T14:10:47.746210Z",
     "iopub.status.idle": "2025-09-16T14:10:50.406350Z",
     "shell.execute_reply": "2025-09-16T14:10:50.405650Z",
     "shell.execute_reply.started": "2025-09-16T14:10:47.746920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing test dataset:   0%|          | 0/2213 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing test dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2213/2213 [00:09<00:00, 222.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5e7a923dd6e4ccb8730eb95230e0c908', 'question': 'å¡åˆ©å‰µç«‹çš„ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨è¦åˆ°ä»€éº¼æ™‚å¾Œæ‰é–‹æ”¾å­˜å–ï¼Ÿ', 'context': 'ç¡¬é«”æ˜¯è£ç½®åœ¨æ©Ÿç®±å…§ä»¥åšå‡ºå€‹äººé›»è…¦ã€‚ç³»çµ±è»Ÿé«”æ˜¯å„²å­˜åœ¨ç¡¬é«”å…§ï¼Œè€Œç³»çµ±è»Ÿé«”å…§å«æœ‰éŸŒé«”ï¼Œä¾‹å¦‚åŸºæœ¬è¼¸å…¥è¼¸å‡ºç³»çµ±ä»¥åŠä½œæ¥­ç³»çµ±ï¼Œé€™äº›è»Ÿé«”ä½¿æ‡‰ç”¨è»Ÿé«”å¯ä»¥æä¾›ä½¿ç”¨è€…æ‰€éœ€çš„åŠŸèƒ½ã€‚ä½œæ¥­ç³»çµ±é€šå¸¸è—‰ç”±åŒ¯æµæ’èˆ‡è£ç½®æºé€šï¼Œé€™éœ€è¦è»Ÿé«”æä¾›é©…å‹•ç¨‹å¼ã€‚è¼¸å…¥å’Œè¼¸å‡ºè£ç½®é€šå¸¸è£åœ¨å¤–éƒ¨çš„ä¸»è¨ˆç®—æ©Ÿæ©Ÿç®±ã€‚è¼¸å…¥è£ç½®ä½¿ç”¨æˆ¶èƒ½å¤ å°‡è³‡è¨Šè¼¸å…¥åˆ°ç³»çµ±ä¸­ï¼Œæˆ–æ§åˆ¶å…¶æ“ä½œã€‚å¤§å¤šæ•¸å€‹äººè¨ˆç®—æ©Ÿçš„é¼ æ¨™å’Œéµç›¤ï¼Œä½†ç­†è¨˜æœ¬é›»è…¦çš„ç³»çµ±é€šå¸¸ä½¿ç”¨çš„é¼ æ¨™è§¸æ‘¸æ¿ä¾†ä»£æ›¿ã€‚å…¶ä»–çš„è¼¸å…¥è£ç½®åŒ…æ‹¬ç¶²è·¯æ”åƒé ­ï¼Œéº¥å…‹é¢¨ï¼ŒéŠæˆ²æ¡¿å’Œåœ–åƒæƒæå„€ã€‚è¼¸å‡ºè£ç½®åœ¨äººé¡å¯è®€çš„å½¢å¼é¡¯ç¤ºè³‡è¨Šã€‚é€™ç¨®è£ç½®å¯ä»¥åŒ…æ‹¬å°è¡¨æ©Ÿï¼ŒéŸ³ç®±ï¼Œé¡¯ç¤ºå™¨æˆ–ç›²æ–‡å£“èŠ±ã€‚è¨ˆç®—æ©Ÿè³‡æ–™å­˜å„²ï¼Œé€šå¸¸ç¨±ç‚ºå­˜å„²æˆ–å…§å­˜ï¼Œæ˜¯æŒ‡è¨ˆç®—æ©Ÿéƒ¨ä»¶å’Œè¨˜éŒ„åª’ä»‹çš„ä¿ç•™æ•¸å­—è³‡æ–™ã€‚è³‡æ–™å­˜å„²æ˜¯ä¸€å€‹æ ¸å¿ƒåŠŸèƒ½å’Œè¨ˆç®—æ©Ÿçš„åŸºæœ¬çµ„æˆéƒ¨åˆ†ã€‚ 1996å¹´ï¼Œå¡åˆ©å‰µç«‹äº†ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨ï¼›åŒå¹´10æœˆé–‹å§‹æ”¶é›†å„²å­˜è³‡æ–™ã€‚ä¸éï¼Œç›´åˆ°2001å¹´é–‹ç™¼äº†ã€Œæ™‚å…‰æ©Ÿã€å‰ï¼Œé€™äº›è³‡æ–™éƒ½ç„¡æ³•å­˜å–ã€‚1999å¹´æœ«æ“´å±•æ”¶é›†ç¯„åœã€‚2012å¹´8æœˆï¼Œå®£å‚³å°‡åœ¨å…¶ç¾å­˜çš„130è¬æª”æ¡ˆçš„ä¸‹è¼‰é¸é …ä¸­åŠ å…¥ä½å…ƒæ´ªæµã€‚å› ç‚ºé€šéå…©å€‹æª”æ¡ˆè³‡æ–™ä¸­å¿ƒå”èª¿ï¼Œé€™æˆç‚ºå¾è©²æª”æ¡ˆé¤¨ä¸‹è¼‰è³‡æ–™çš„æœ€å¿«æ–¹æ³•ã€‚2013å¹´11æœˆ6æ—¥ï¼Œæª”æ¡ˆé¤¨åœ¨é‡Œå¥‡è’™å¾·å€çš„ç¸½éƒ¨å¤±ç«ï¼Œæå£äº†è¨±å¤šè£ç½®å’Œä¸€äº›é™„è¿‘çš„å…¬å¯“ï¼Œé è¨ˆæå¤±é”åˆ°60è¬ç¾å…ƒã€‚æ™‚å…‰æ©Ÿæ˜¯ç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨æœ€é‡è¦çš„æœå‹™ä¹‹ä¸€ã€‚æ™‚å…‰æ©Ÿå…è¨±äººå€‘å»æœå°‹å’Œå­˜å–å…¶ç¶²é å­˜æª”ã€‚åœ¨ä¸€äº›åœ‹å®¶å’Œåœ°å€ï¼Œæ™‚å…‰æ©Ÿé€™å€‹è¡“èªçš„ä½¿ç”¨å·²ç¶“éå¸¸æ™®éï¼Œã€Œæ™‚å…‰æ©Ÿã€å’Œã€Œç¶²éš›ç¶²è·¯æª”æ¡ˆé¤¨ã€ç”šè‡³é–‹å§‹è¢«ç•¶åšåŒç¾©è©ä½¿ç”¨ã€‚ 1994å¹´10æœˆï¼Œå…¨çƒè³‡è¨Šç¶²å”æœƒåœ¨éº»çœç†å·¥å­¸é™¢é›»è…¦ç§‘å­¸å¯¦é©—å®¤æˆç«‹ï¼Œå»ºç«‹è€…æ˜¯å…¨çƒè³‡è¨Šç¶²çš„ç™¼æ˜è€…æå§†Â·æŸå…§èŒ²-æã€‚åˆ°äº†1994å¹´åº•ï¼Œå…¨çƒç¶²ç«™æ•¸é‡ä¾ç„¶ç›¸å°è¼ƒå°‘ï¼Œä½†æ˜¯å¾ˆå¤šè‘—åç¶²ç«™å·²ç¶“ç›¸ç•¶æ´»èºï¼Œé€™äº›ç¶²ç«™å·²ç¶“é ç¤ºæˆ–è€…å•Ÿç™¼äº†ç•¶ä»Šæœ€æµè¡Œçš„æœå‹™ã€‚é€éç¶²éš›ç¶²è·¯é€£æ¥ï¼Œä¸–ç•Œå„åœ°ä¹Ÿå»ºç«‹äº†å…¶ä»–ç¶²ç«™ã€‚é€™ä¿ƒé€²äº†å”è­°å’Œæ ¼å¼åŒ–çš„åœ‹éš›æ¨™æº–ç™¼å±•ã€‚æŸå…§èŒ²-æç¹¼çºŒåƒèˆ‡æŒ‡å°å…¨çƒè³‡è¨Šç¶²æ¨™æº–çš„ç™¼å±•ï¼Œä¾‹å¦‚æ¨™è¨˜å¼èªè¨€ä¾†çµ„æˆç¶²é å’Œä»–ä¸»å¼µçš„èªç¾©ç¶²é¡˜æ™¯ã€‚å…¨çƒè³‡è¨Šç¶²é€éæ˜“æ–¼ä½¿ç”¨å’Œéˆæ´»çš„æ ¼å¼åœ¨ç¶²éš›ç¶²è·¯ä¸Šå‚³æ’­è³‡è¨Šï¼Œå› æ­¤å°æ–¼ç¶²éš›ç¶²è·¯çš„æ™®åŠç™¼æ®äº†é‡è¦çš„ä½œç”¨ã€‚é›–ç„¶é€™å…©å€‹è¡“èªæœ‰æ™‚è¢«äººå€‘å»£æ³›ä½¿ç”¨ï¼Œä½†å…¨çƒè³‡è¨Šç¶²ä¸¦ä¸æ˜¯ç¶²éš›ç¶²è·¯çš„ä»£åè©ã€‚å…¨çƒè³‡è¨Šç¶²æ˜¯ä¸€å€‹è³‡è¨Šç©ºé–“ï¼ŒåŒ…å«è¶…é€£çµæ–‡ä»¶å’Œå…¶ä»–è³‡æºï¼Œä¸¦ä¸”ç”±å®ƒå€‘çš„URIsæ¨™è­˜ã€‚ç¶²éš›ç¶²è·¯å’Œå…¨çƒè³‡è¨Šç¶²é€™å…©å€‹è©é€šå¸¸æ²’æœ‰å¤šå°‘å€åˆ¥ã€‚ä½†æ˜¯ï¼Œå…©è€…ä¸¦ä¸ç›¸åŒã€‚ç¶²éš›ç¶²è·¯æ˜¯ä¸€å€‹å…¨çƒäº’ç›¸é€£æ¥çš„é›»è…¦ç¶²è·¯ç³»çµ±ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå…¨çƒè³‡è¨Šç¶²æ˜¯é€éè¶…é€£çµå’ŒURIsé€£æ¥çš„å…¨çƒæ”¶é›†æª”æ¡ˆå’Œå…¶ä»–è³‡æºã€‚å…¨çƒè³‡è¨Šç¶²è³‡æºé€šå¸¸ä½¿ç”¨HTTPå­˜å–ï¼Œé€™æ˜¯è¨±å¤šç¶²éš›ç¶²è·¯é€šè¨Šå”è­°çš„å…¶ä¸­ä¹‹ä¸€ã€‚è‹¥è¦é€²å…¥å…¨çƒè³‡è¨Šç¶²ä¸Šä¸€å€‹ç¶²é ï¼Œæˆ–è€…å…¶ä»–ç¶²è·¯è³‡æºçš„æ™‚å€™ï¼Œé€šå¸¸éœ€ç€è¦½å™¨ä¸Šéµå…¥ä½ æƒ³å­˜å–ç¶²é çš„çµ±ä¸€è³‡æºå®šä½ç¬¦ï¼Œæˆ–è€…é€šéè¶…é€£çµæ–¹å¼é€£çµåˆ°é‚£å€‹ç¶²é æˆ–ç¶²è·¯è³‡æºã€‚é€™ä¹‹å¾Œçš„å·¥ä½œé¦–å…ˆæ˜¯URLçš„ä¼ºæœå™¨åéƒ¨åˆ†ï¼Œè¢«åç‚ºåŸŸåç³»çµ±çš„åˆ†å¸ƒæ–¼å…¨çƒçš„ç¶²éš›ç¶²è·¯è³‡æ–™åº«è§£æï¼Œä¸¦æ ¹æ“šè§£æçµæœæ±ºå®šé€²å…¥å“ªä¸€å€‹IPä½å€ã€‚æ¥ä¸‹ä¾†çš„æ­¥é©Ÿæ˜¯ç‚ºæ‰€è¦å­˜å–çš„ç¶²é ï¼Œå‘åœ¨é‚£å€‹IPä½å€å·¥ä½œçš„ä¼ºæœå™¨å‚³é€ä¸€å€‹HTTPè«‹æ±‚ã€‚åœ¨é€šå¸¸æƒ…æ³ä¸‹ï¼ŒHTMLæ–‡å­—ã€åœ–ç‰‡å’Œæ§‹æˆè©²ç¶²é çš„ä¸€åˆ‡å…¶ä»–æª”æ¡ˆå¾ˆå¿«æœƒè¢«é€ä¸€è«‹æ±‚ä¸¦è¡Œé€å›ç”¨æˆ¶ã€‚ç¶²è·¯ç€è¦½å™¨æ¥ä¸‹ä¾†çš„å·¥ä½œæ˜¯æŠŠHTMLã€CSSå’Œå…¶ä»–æ¥å—åˆ°çš„æª”æ¡ˆæ‰€æè¿°çš„å…§å®¹ï¼ŒåŠ ä¸Šåœ–åƒã€é€£çµå’Œå…¶ä»–å¿…é ˆçš„è³‡æºï¼Œé¡¯ç¤ºçµ¦ç”¨æˆ¶ã€‚é€™äº›å°±æ§‹æˆäº†ä½ æ‰€çœ‹åˆ°çš„ç¶²é ã€‚å¤§å¤šæ•¸çš„ç¶²é è‡ªèº«åŒ…å«æœ‰è¶…é€£çµæŒ‡å‘å…¶ä»–ç›¸é—œç¶²é ï¼Œå¯èƒ½é‚„æœ‰ä¸‹è¼‰ã€æºæ–‡ç»ã€å®šç¾©å’Œå…¶ä»–ç¶²è·¯è³‡æºã€‚åƒé€™æ¨£é€šéè¶…é€£çµï¼ŒæŠŠæœ‰ç”¨çš„ç›¸é—œè³‡æºçµ„ç¹”åœ¨ä¸€èµ·çš„é›†åˆï¼Œå°±å½¢æˆäº†ä¸€å€‹æ‰€è¬‚çš„è³‡è¨Šçš„ç¶²ã€‚é€™å€‹ç¶²åœ¨ç¶²éš›ç¶²è·¯ä¸Šè¢«æ–¹ä¾¿ä½¿ç”¨ï¼Œå°±æ§‹æˆäº†æœ€æ—©åœ¨1990å¹´ä»£åˆæå§†Â·æŸå…§èŒ²-ææ‰€èªªçš„å…¨çƒè³‡è¨Šç¶²ã€‚ ç¾ä»£æ™‚æœŸçš„é–‹å§‹ä»£è¡¨äº†æ­æ´²æ–‡è—å¾©èˆˆçš„çµæŸã€‚ç¢ºåˆ‡çš„æ™‚é–“é»æœƒä¾æ“šå„å€‹é ˜åŸŸçš„ç•Œå®šè€Œæœ‰æ‰€ä¸åŒï¼Œæ¯”å¦‚èªªï¼Œä¸€å€‹æ­·å²å­¸å®¶å¯èƒ½æœƒå°‡ç¾ä»£çš„é–‹å§‹æ”¾åœ¨1650å¹´ï¼Œè€Œä¸€å€‹éŸ³æ¨‚å®¶å‰‡å¯èƒ½æœƒæŠŠæ™‚é–“æ”¾åœ¨éŸ³æ¨‚çš„æµªæ¼«æ™‚æœŸçµæŸä¹‹å¾Œï¼Œä¹Ÿå°±æ˜¯å¤§ç´„1900å¹´ã€‚ç•¶ä»£å²å­¸è€…ç¿’æ…£æŠŠä¸€æˆ°çˆ†ç™¼ï¼Œä½œç‚ºè¿‘ä»£çš„çµ‚çµã€ç¾ä»£çš„é–‹ç«¯ã€‚åŒæ¨£åœ°ï¼Œä¾æ“šå„å€‹é ˜åŸŸçš„ä¸åŒè§€é»ï¼Œç¾ä»£æ™‚æœŸå¯èƒ½æœƒè¢«ç•Œå®šç‚ºä¸€ç›´å»¶ä¼¸åˆ°æˆ‘å€‘ç¾åœ¨æ‰€è™•çš„æ™‚é–“ï¼Œæˆ–è€…ä¹Ÿæœ‰å¯èƒ½è¢«èªç‚ºçµæŸæ–¼å¾Œç¾ä»£ä¸»ç¾©çš„é–‹å§‹ï¼Œå¯èƒ½åœ¨1960å¹´ä»£åˆ°1980å¹´ä»£åˆæœŸä¹‹é–“çš„ä»»ä½•ä¸€é»ã€‚å¦‚æœæ˜¯å°‡ç¾ä»£å®šç¾©ç‚ºåœ¨å¾Œç¾ä»£ä¹‹å‰çš„è©±ï¼Œé‚£éº¼å®ƒå¯èƒ½ç‰¹åˆ¥æŒ‡æ¶‰äº†ç¾ä»£ä¸»ç¾©ã€‚æœ‰çš„è§€é»å‰‡èªç‚ºï¼Œå¾Œç¾ä»£ä¸»ç¾©åªä¸éæ˜¯ç¾ä»£ä¸»ç¾©æœ¬èº«æœ€æ™šä¸€å€‹æ™‚æœŸçš„ç™¼å±•è€Œå·²ã€‚ '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ========= Preprocessing (Inference) =========\n",
    "def prepare_validation_features(examples):\n",
    "    # å»é™¤é–‹é ­ç©ºç™½\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # tokenize question-context pair\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,   # ä¿ç•™ offset_mapping\n",
    "        padding=\"max_length\" if args.pad_to_max_length else False,\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # åªä¿ç•™ context éƒ¨åˆ†çš„ offset_mapping\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "test_examples = raw_datasets[\"test\"]\n",
    "test_dataset = test_examples.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Tokenizing test dataset\",\n",
    ")\n",
    "\n",
    "# å»ºç«‹ dataset çµ¦ modelï¼ˆå»æ‰ offset_mappingï¼‰\n",
    "test_dataset_for_model = test_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "\n",
    "# ========= DataLoader =========\n",
    "if args.pad_to_max_length:\n",
    "    data_collator = default_data_collator\n",
    "else:\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset_for_model,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=args.per_device_eval_batch_size,\n",
    ")\n",
    "\n",
    "print(test_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:10:50.408659Z",
     "iopub.status.busy": "2025-09-16T14:10:50.408405Z",
     "iopub.status.idle": "2025-09-16T14:29:36.582838Z",
     "shell.execute_reply": "2025-09-16T14:29:36.582032Z",
     "shell.execute_reply.started": "2025-09-16T14:10:50.408640Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running Inference *****\n",
      "  Num examples = 10299\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10299/10299 [02:19<00:00, 73.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing 2213 example predictions split into 10299 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2213/2213 [00:56<00:00, 39.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# ===== Inference =====\n",
    "print(\"***** Running Inference *****\")\n",
    "print(f\"  Num examples = {len(test_dataset)}\")\n",
    "print(f\"  Batch size = {args.per_device_eval_batch_size}\")\n",
    "\n",
    "all_start_logits = []\n",
    "all_end_logits = []\n",
    "model.eval()\n",
    "\n",
    "for step, batch in enumerate(tqdm(test_dataloader, desc=\"Inference\", disable=not accelerator.is_local_main_process)):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.to(model.device) for k, v in batch.items()})\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "        if not args.pad_to_max_length:\n",
    "            start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n",
    "            end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n",
    "\n",
    "        all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n",
    "        all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n",
    "\n",
    "\n",
    "# ===== æ•´ç† logits =====\n",
    "\n",
    "# Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n",
    "def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n",
    "    step = 0\n",
    "    # create a numpy array and fill it with -100.\n",
    "    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n",
    "    # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather_for_metrics\n",
    "    for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n",
    "        # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n",
    "        # And after every iteration we have to change the step\n",
    "\n",
    "        batch_size = output_logit.shape[0]\n",
    "        cols = output_logit.shape[1]\n",
    "\n",
    "        if step + batch_size < len(dataset):\n",
    "            logits_concat[step : step + batch_size, :cols] = output_logit\n",
    "        else:\n",
    "            logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n",
    "\n",
    "        step += batch_size\n",
    "\n",
    "    return logits_concat\n",
    "\n",
    "max_len = max(x.shape[1] for x in all_start_logits)\n",
    "start_logits_concat = create_and_fill_np_array(all_start_logits, test_dataset, max_len)\n",
    "end_logits_concat = create_and_fill_np_array(all_end_logits, test_dataset, max_len)\n",
    "\n",
    "# æ¸…ç†æš«å­˜\n",
    "del all_start_logits\n",
    "del all_end_logits\n",
    "\n",
    "# ===== å¾Œè™•ç†ï¼Œè½‰æˆæ–‡å­—ç­”æ¡ˆ =====\n",
    "outputs_numpy = (start_logits_concat, end_logits_concat)\n",
    "predictions = postprocess_qa_predictions(\n",
    "    examples=test_examples,\n",
    "    features=test_dataset,\n",
    "    predictions=outputs_numpy,\n",
    "    version_2_with_negative=False,\n",
    "    n_best_size=args.n_best_size,\n",
    "    max_answer_length=args.max_answer_length,\n",
    "    null_score_diff_threshold=0.0,\n",
    "    output_dir=None,   # ä¸ç”¨è¼¸å‡ºåˆ°ä¸­é–“æª”\n",
    "    prefix=\"test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-16T14:29:36.584367Z",
     "iopub.status.busy": "2025-09-16T14:29:36.583754Z",
     "iopub.status.idle": "2025-09-16T14:29:36.594578Z",
     "shell.execute_reply": "2025-09-16T14:29:36.593697Z",
     "shell.execute_reply.started": "2025-09-16T14:29:36.584339Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to output/prediction.csv\n"
     ]
    }
   ],
   "source": [
    "# ===== å­˜æˆ CSV =====\n",
    "if accelerator.is_main_process:\n",
    "    with open(args.output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"id\", \"answer\"])\n",
    "        for qid, ans in predictions.items():\n",
    "            writer.writerow([qid, ans])\n",
    "\n",
    "    print(f\"Saved predictions to {args.output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13653249,
     "sourceId": 114453,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "3.10.18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
