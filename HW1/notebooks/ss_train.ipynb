{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":114453,"databundleVersionId":13653249,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":13087344,"sourceType":"datasetVersion","datasetId":8289365},{"sourceId":582897,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":435262,"modelId":452077}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi\n!pip install datasets evaluate numpy torch accelerate tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-16T17:58:52.361803Z","iopub.execute_input":"2025-09-16T17:58:52.362129Z","iopub.status.idle":"2025-09-16T18:00:09.656238Z","shell.execute_reply.started":"2025-09-16T17:58:52.362098Z","shell.execute_reply":"2025-09-16T18:00:09.655428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!/usr/bin/env python\n# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning a ü§ó Transformers model for question answering using ü§ó Accelerate.\n\"\"\"\n# You can also adapt this script on your own question answering task. Pointers for this are left as comments.\n\nimport argparse\nimport collections\nimport json\nimport logging\nimport math\nimport os\nimport random\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom typing import Optional\n\nimport datasets\nimport evaluate\nimport numpy as np\nimport torch\nfrom accelerate import Accelerator\nfrom accelerate.utils import set_seed\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\nimport transformers\nfrom transformers import (\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    AutoConfig,\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    EvalPrediction,\n    SchedulerType,\n    default_data_collator,\n    get_scheduler,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:00:09.658264Z","iopub.execute_input":"2025-09-16T18:00:09.658552Z","iopub.status.idle":"2025-09-16T18:00:39.471513Z","shell.execute_reply.started":"2025-09-16T18:00:09.658526Z","shell.execute_reply":"2025-09-16T18:00:39.470853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"args = SimpleNamespace(\n    train_file=\"/kaggle/input/ntu-adl-2025-hw-1/train.json\",\n    validation_file=\"/kaggle/input/ntu-adl-2025-hw-1/valid.json\",\n    context_file=\"/kaggle/input/ntu-adl-2025-hw-1/context.json\",\n    max_seq_length=512,\n    pad_to_max_length=False,\n    model_name_or_path=\"hfl/chinese-macbert-base\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    learning_rate=3e-5,\n    num_train_epochs=2,\n    max_train_steps=None,\n    gradient_accumulation_steps=2,\n    lr_scheduler_type=SchedulerType.LINEAR, # choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n    output_dir=\"/kaggle/working/\",\n    seed=1234,\n    doc_stride=128,\n    n_best_size=20,\n    max_answer_length=30,\n)\n\nprint(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:00:39.543040Z","iopub.execute_input":"2025-09-16T18:00:39.543286Z","iopub.status.idle":"2025-09-16T18:00:39.562032Z","shell.execute_reply.started":"2025-09-16T18:00:39.543259Z","shell.execute_reply":"2025-09-16T18:00:39.561386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def postprocess_qa_predictions(\n    examples,\n    features,\n    predictions: tuple[np.ndarray, np.ndarray],\n    version_2_with_negative: bool = False,\n    n_best_size: int = 20,\n    max_answer_length: int = 30,\n    null_score_diff_threshold: float = 0.0,\n    output_dir: Optional[str] = None,\n    prefix: Optional[str] = None,\n    log_level: Optional[int] = logging.WARNING,\n):\n    if len(predictions) != 2:\n        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n    all_start_logits, all_end_logits = predictions\n\n    if len(predictions[0]) != len(features):\n        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_prediction = None\n        prelim_predictions = []\n\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n            # available in the current feature.\n            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n\n            # Update minimum null prediction.\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n                min_null_prediction = {\n                    \"offsets\": (0, 0),\n                    \"score\": feature_null_score,\n                    \"start_logit\": start_logits[0],\n                    \"end_logit\": end_logits[0],\n                }\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or len(offset_mapping[start_index]) < 2\n                        or offset_mapping[end_index] is None\n                        or len(offset_mapping[end_index]) < 2\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n                    # Don't consider answer that don't have the maximum context available (if such information is\n                    # provided).\n                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n                        continue\n\n                    prelim_predictions.append(\n                        {\n                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"start_logit\": start_logits[start_index],\n                            \"end_logit\": end_logits[end_index],\n                        }\n                    )\n\n        # Only keep the best `n_best_size` predictions.\n        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n\n        # Use the offsets to gather the answer text in the original context.\n        context = example[\"context\"]\n        for pred in predictions:\n            offsets = pred.pop(\"offsets\")\n            pred[\"text\"] = context[offsets[0] : offsets[1]]\n\n        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n        # failure.\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n\n        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n        # the LogSumExp trick).\n        scores = np.array([pred.pop(\"score\") for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n\n        # Include the probabilities in our predictions.\n        for prob, pred in zip(probs, predictions):\n            pred[\"probability\"] = prob\n\n        # Pick the best prediction. If the null answer is not possible, this is easy.\n        all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n\n        # Make `predictions` JSON-serializable by casting np.float back to float.\n        all_nbest_json[example[\"id\"]] = [\n            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n            for pred in predictions\n        ]\n\n    # If we have an output_dir, let's save all those dicts.\n    if output_dir is not None:\n        if not os.path.isdir(output_dir):\n            raise OSError(f\"{output_dir} is not a directory.\")\n\n        prediction_file = os.path.join(\n            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n        )\n        nbest_file = os.path.join(\n            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n        )\n\n        print(f\"Saving predictions to {prediction_file}.\")\n        with open(prediction_file, \"w\") as writer:\n            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n        print(f\"Saving nbest_preds to {nbest_file}.\")\n        with open(nbest_file, \"w\") as writer:\n            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n\n    return all_predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:00:39.472287Z","iopub.execute_input":"2025-09-16T18:00:39.472816Z","iopub.status.idle":"2025-09-16T18:00:39.490015Z","shell.execute_reply.started":"2025-09-16T18:00:39.472796Z","shell.execute_reply":"2025-09-16T18:00:39.488958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_prefixed_metrics(results, output_dir, file_name: str = \"all_results.json\", metric_key_prefix: str = \"eval\"):\n    \"\"\"\n    Save results while prefixing metric names.\n\n    Args:\n        results: (:obj:`dict`):\n            A dictionary of results.\n        output_dir: (:obj:`str`):\n            An output directory.\n        file_name: (:obj:`str`, `optional`, defaults to :obj:`all_results.json`):\n            An output file name.\n        metric_key_prefix: (:obj:`str`, `optional`, defaults to :obj:`eval`):\n            A metric name prefix.\n    \"\"\"\n    # Prefix all keys with metric_key_prefix + '_'\n    for key in list(results.keys()):\n        if not key.startswith(f\"{metric_key_prefix}_\"):\n            results[f\"{metric_key_prefix}_{key}\"] = results.pop(key)\n\n    with open(os.path.join(output_dir, file_name), \"w\") as f:\n        json.dump(results, f, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:00:39.490879Z","iopub.execute_input":"2025-09-16T18:00:39.491164Z","iopub.status.idle":"2025-09-16T18:00:39.541692Z","shell.execute_reply.started":"2025-09-16T18:00:39.491145Z","shell.execute_reply":"2025-09-16T18:00:39.540967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the accelerator.\naccelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)\n\n# If passed along, set the training seed now.\nif args.seed is not None:\n    set_seed(args.seed)\n\n# Handle the repository creation\nif accelerator.is_main_process:\n    if args.output_dir is not None:\n        os.makedirs(args.output_dir, exist_ok=True)\naccelerator.wait_for_everyone()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:00:39.562769Z","iopub.execute_input":"2025-09-16T18:00:39.563104Z","iopub.status.idle":"2025-09-16T18:00:39.612539Z","shell.execute_reply.started":"2025-09-16T18:00:39.563082Z","shell.execute_reply":"2025-09-16T18:00:39.611678Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport datasets\n\n# ËºâÂÖ• context ÊÆµËêΩ\nwith open(args.context_file, \"r\", encoding=\"utf-8\") as f:\n    contexts = json.load(f)\n\ndef load_span_selection(filename, contexts):\n    \"\"\"ËÆÄÂèñ span selection datasetÔºåÂõûÂÇ≥ list of dict\"\"\"\n    with open(filename, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    examples = []\n    for ex in data:\n        context_text = contexts[ex[\"relevant\"]]  # Áî® relevant id ÊâæÊÆµËêΩ\n        answer_text = ex[\"answer\"][\"text\"]\n        answer_start = ex[\"answer\"][\"start\"]\n\n        examples.append({\n            \"id\": ex[\"id\"],\n            \"question\": ex[\"question\"],\n            \"context\": context_text,\n            \"answers\": {\n                \"text\": [answer_text],\n                \"answer_start\": [answer_start],\n            }\n        })\n    return examples\n\n# === Âª∫Á´ã DatasetDict ===\ndataset_splits = {}\nif args.train_file is not None:\n    dataset_splits[\"train\"] = load_span_selection(args.train_file, contexts)\nif args.validation_file is not None:\n    dataset_splits[\"validation\"] = load_span_selection(args.validation_file, contexts)\n\nraw_datasets = datasets.DatasetDict({\n    split: datasets.Dataset.from_list(data)\n    for split, data in dataset_splits.items()\n})\n\nprint(raw_datasets)\nprint(raw_datasets[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:00:39.614754Z","iopub.execute_input":"2025-09-16T18:00:39.615031Z","iopub.status.idle":"2025-09-16T18:00:40.914761Z","shell.execute_reply.started":"2025-09-16T18:00:39.615011Z","shell.execute_reply":"2025-09-16T18:00:40.913943Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========= Load pretrained model and tokenizer =========\nconfig = AutoConfig.from_pretrained(args.model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\nmodel = AutoModelForQuestionAnswering.from_pretrained(\n    args.model_name_or_path,\n    from_tf=bool(\".ckpt\" in args.model_name_or_path),\n    config=config,\n)\n\n# ========= Dataset column names =========\ncolumn_names = raw_datasets[\"train\"].column_names\nquestion_column_name = \"question\" if \"question\" in column_names else column_names[0]\ncontext_column_name = \"context\" if \"context\" in column_names else column_names[1]\nanswer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n\npad_on_right = tokenizer.padding_side == \"right\"\nmax_seq_length = min(args.max_seq_length, tokenizer.model_max_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:00:40.915660Z","iopub.execute_input":"2025-09-16T18:00:40.915999Z","iopub.status.idle":"2025-09-16T18:01:27.123198Z","shell.execute_reply.started":"2025-09-16T18:00:40.915967Z","shell.execute_reply":"2025-09-16T18:01:27.122207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========= Preprocessing (Train) =========\ndef prepare_train_features(examples):\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n\n    tokenized_examples = tokenizer(\n        examples[question_column_name if pad_on_right else context_column_name],\n        examples[context_column_name if pad_on_right else question_column_name],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\" if args.pad_to_max_length else False,\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id) if tokenizer.cls_token_id in input_ids else 0\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = examples[answer_column_name][sample_index]\n\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            token_start_index = next(idx for idx, s in enumerate(sequence_ids) if s == (1 if pad_on_right else 0))\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples\n\nwith accelerator.main_process_first():\n    train_dataset = raw_datasets[\"train\"].map(\n        prepare_train_features,\n        batched=True,\n        remove_columns=column_names,\n        desc=\"Running tokenizer on train dataset\",\n    )\n\n# ========= Preprocessing (Validation) =========\ndef prepare_validation_features(examples):\n    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n\n    tokenized_examples = tokenizer(\n        examples[question_column_name if pad_on_right else context_column_name],\n        examples[context_column_name if pad_on_right else question_column_name],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\" if args.pad_to_max_length else False,\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples\n\neval_examples = raw_datasets[\"validation\"]\nwith accelerator.main_process_first():\n    eval_dataset = eval_examples.map(\n        prepare_validation_features,\n        batched=True,\n        remove_columns=column_names,\n        desc=\"Running tokenizer on validation dataset\",\n    )\n\n# ========= DataLoaders =========\nif args.pad_to_max_length:\n    data_collator = default_data_collator\nelse:\n    pad_to_multiple_of = 16 if accelerator.mixed_precision == \"fp8\" else (8 if accelerator.mixed_precision != \"no\" else None)\n    data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size)\neval_dataset_for_model = eval_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\neval_dataloader = DataLoader(eval_dataset_for_model, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n\n# ========= Post-processing =========\ndef post_processing_function(examples, features, predictions, stage=\"eval\"):\n    predictions = postprocess_qa_predictions(\n        examples=examples,\n        features=features,\n        predictions=predictions,\n        version_2_with_negative=False,\n        n_best_size=args.n_best_size,\n        max_answer_length=args.max_answer_length,\n        null_score_diff_threshold=0.0,\n        output_dir=args.output_dir,\n        prefix=stage,\n    )\n    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n\nmetric = evaluate.load(\"squad\")\n\n# Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\ndef create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n    step = 0\n    # create a numpy array and fill it with -100.\n    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n    # Now since we have create an array now we will populate it with the outputs gathered using accelerator.gather_for_metrics\n    for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n        # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n        # And after every iteration we have to change the step\n\n        batch_size = output_logit.shape[0]\n        cols = output_logit.shape[1]\n\n        if step + batch_size < len(dataset):\n            logits_concat[step : step + batch_size, :cols] = output_logit\n        else:\n            logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n\n        step += batch_size\n\n    return logits_concat\n\n# ========= Optimizer & Scheduler =========\nno_decay = [\"bias\", \"LayerNorm.weight\"]\noptimizer_grouped_parameters = [\n    {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n]\noptimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n\nnum_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\nif args.max_train_steps is None:\n    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    name=args.lr_scheduler_type,\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=args.max_train_steps,\n)\n\n# ========= Accelerator prepare =========\nmodel, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n)\n\n# Recalculate total training steps\nnum_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\nargs.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n# ========= Training info =========\ntotal_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:01:27.124161Z","iopub.execute_input":"2025-09-16T18:01:27.124485Z","iopub.status.idle":"2025-09-16T18:01:56.033412Z","shell.execute_reply.started":"2025-09-16T18:01:27.124458Z","shell.execute_reply":"2025-09-16T18:01:56.032611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"***** Running training *****\")\nprint(f\"  Num examples = {len(train_dataset)}\")\nprint(f\"  Num Epochs = {args.num_train_epochs}\")\nprint(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\nprint(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\nprint(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\nprint(f\"  Total optimization steps = {args.max_train_steps}\")\n\n# Progress bar (only main processÈ°ØÁ§∫)\nprogress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\ncompleted_steps = 0\nstarting_epoch = 0\n\n# Â¶ÇÊûúÊòØÂæû checkpoint ÊÅ¢Âæ©ÔºåÊõ¥Êñ∞ÈÄ≤Â∫¶Ê¢ù\nprogress_bar.update(completed_steps)\n\n# ====== Logs ======\nloss_history = []\nmetric_history = []\nsteps_record = []\n\n# Progress bar (only main processÈ°ØÁ§∫)\nprogress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\ncompleted_steps = 0\nstarting_epoch = 0\n\nprogress_bar.update(completed_steps)\n\n# -------- Training Loop --------\nfor epoch in range(starting_epoch, args.num_train_epochs):\n    model.train()\n    running_loss = 0.0  \n    log_interval = 1000\n    eval_interval = args.max_train_steps // 5   # Ëá≥Â∞ë 5 ÂÄãÈªû\n\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss\n\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n        running_loss += loss.item()\n\n        # ÊØè log_interval steps Ë®òÈåÑ loss\n        if (step + 1) % log_interval == 0 and accelerator.is_local_main_process:\n            avg_loss = running_loss / log_interval\n            print(f\"Epoch {epoch} | Step {step+1} | Avg Loss: {avg_loss:.4f}\")\n            loss_history.append(avg_loss)\n            steps_record.append(completed_steps)\n            running_loss = 0.0  \n\n        # ÊØè eval_interval steps ÂÅö evaluation\n        if completed_steps % eval_interval == 0 and completed_steps > 0 and accelerator.is_local_main_process:\n            model.eval()\n            all_start_logits, all_end_logits = [], []\n            for e_step, e_batch in enumerate(eval_dataloader):\n                with torch.no_grad():\n                    outputs = model(**e_batch)\n                    start_logits = outputs.start_logits\n                    end_logits = outputs.end_logits\n\n                if not args.pad_to_max_length:\n                    start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n                    end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n\n                all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n\n            max_len = max(x.shape[1] for x in all_start_logits)\n            start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n            end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n            outputs_numpy = (start_logits_concat, end_logits_concat)\n\n            prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n            eval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\n\n            metric_history.append(eval_metric[\"exact_match\"])\n            print(f\"Step {completed_steps} | EM = {eval_metric['exact_match']:.2f}\")\n\n            del all_start_logits, all_end_logits\n            model.train()\n\n        if accelerator.sync_gradients:\n            progress_bar.update(1)\n            completed_steps += 1\n\n        if completed_steps >= args.max_train_steps:\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:01:56.034364Z","iopub.execute_input":"2025-09-16T18:01:56.034676Z","iopub.status.idle":"2025-09-16T20:59:24.772131Z","shell.execute_reply.started":"2025-09-16T18:01:56.034650Z","shell.execute_reply":"2025-09-16T20:59:24.771197Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------- Evaluation Loop --------\nprint(\"***** Running Evaluation *****\")\nprint(f\"  Num examples = {len(eval_dataset)}\")\nprint(f\"  Batch size = {args.per_device_eval_batch_size}\")\n\nall_start_logits = []\nall_end_logits = []\nmodel.eval()\n\nfor step, batch in enumerate(eval_dataloader):\n    with torch.no_grad():\n        outputs = model(**batch)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n\n        if not args.pad_to_max_length:\n            start_logits = accelerator.pad_across_processes(start_logits, dim=1, pad_index=-100)\n            end_logits = accelerator.pad_across_processes(end_logits, dim=1, pad_index=-100)\n\n        all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n        all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n\nmax_len = max(x.shape[1] for x in all_start_logits)\n\n# concatenate results\nstart_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\nend_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n\n# clean up\ndel all_start_logits\ndel all_end_logits\n\n# post-processing + metrics\noutputs_numpy = (start_logits_concat, end_logits_concat)\nprediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\neval_metric = metric.compute(predictions=prediction.predictions, references=prediction.label_ids)\nprint(f\"Evaluation metrics: {eval_metric}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------- Save Model --------\nif args.output_dir is not None:\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    unwrapped_model.save_pretrained(\n        args.output_dir, \n        is_main_process=accelerator.is_main_process, \n        save_function=accelerator.save\n    )\n    if accelerator.is_main_process:\n        tokenizer.save_pretrained(args.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/working.zip /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T20:59:24.773130Z","iopub.execute_input":"2025-09-16T20:59:24.773436Z","iopub.status.idle":"2025-09-16T20:59:48.318039Z","shell.execute_reply.started":"2025-09-16T20:59:24.773412Z","shell.execute_reply":"2025-09-16T20:59:48.316940Z"}},"outputs":[],"execution_count":null}]}